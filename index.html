<!DOCTYPE HTML>

    <html lang="zh-Hans">
  
<head>
  <meta charset="utf-8">
  
  <title>CastellanZhang&#39;s blog</title>
  <meta name="author" content="CastellanZhang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="CastellanZhang&#39;s blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <meta http-equiv="Content-Language" content="zh-Hans"/>
  

  <link href="/img/favicon.png" rel="icon">
  
    <link rel="apple-touch-icon" href="/img/apple-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/img/apple-icon.png">
    

  <link rel="alternate" href="/atom.xml" title="CastellanZhang&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  
  <style type="text/css">
  /* Tim Pietrusky advanced checkbox hack (Android <= 4.1.2) */
body{ -webkit-animation: bugfix infinite 1s; }
@-webkit-keyframes bugfix { from {padding:0;} to {padding:0;} }

  
  <!-- Chinese readability improvements -->
    article {font-weight: 400;letter-spacing: .01rem;}
    article .entry{line-height:2;}
  

  
    article .post-content-index .entry{max-height: 550px; overflow:hidden;}
  
</style>

  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'null', 'auto');
  ga('send', 'pageview');
 
</script>




  
    <!-- 360 Font and Baidu CDN in China -->
    
      <link href='http://fonts.useso.com/css?family=Open+Sans:300,400|Playball' rel='stylesheet' type='text/css'>
    
  <link href='http://apps.bdimg.com/libs/fontawesome/4.1.0/css/font-awesome.css' rel='stylesheet' type='text/css'>
  <script src="http://libs.baidu.com/jquery/1.11.1/jquery.min.js"></script>
  



</head>


<body>
  <header id="header" class="inner"><div class="padding">
	<div class="alignleft logo">
	  <h1><a href="/">CastellanZhang&#39;s blog</a></h1>
	</div>
	<nav id="main-nav" class="alignright">
		<input type="checkbox" id="toggle" />
		<label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu" onclick><i class="fa fa-bars"></i></label>
	  <ul class="menu">
	    
	      <li><a href="/">Home</a></li>
	    
	      <li><a href="/archives">Archives</a></li>
	    
	    
	  </ul>
	</nav>
	<div class="clearfix"></div>
</div>
</header>
  <div id="page-heading-wrap">
  	<div class="inner">
      <div class="padding">
    		
          <h2></h2>
        
      </div>
  	</div>
  </div>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper" class="padding">
  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/10/21/ocpc_roi/">Paper Reading: OCPC, ROI</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-10-20T17:06:51.000Z">Oct 21 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="u8BBA_u6587_u5B66_u4E60_uFF1AOCPC_2C_ROI"><a href="#u8BBA_u6587_u5B66_u4E60_uFF1AOCPC_2C_ROI" class="headerlink" title="论文学习：OCPC, ROI"></a>论文学习：OCPC, ROI</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　翻 KDD2017的论文，又瞅见了阿里的这篇Optimized Cost per Click in Taobao Display Advertising，好文章常看常新，重看一遍写个学习笔记，同时把论文里省略的证明补充一下。</p>
<p>　　这篇论文详细介绍了阿里展示广告的OCPC（Optimized Cost per Click）机制，以手机淘宝首页的Banner广告（Banner CPC Ads，广告可以是商品、店铺或品牌）和猜你喜欢版块的广告（Item CPC Ads，猜你喜欢200个推荐位里包含3个广告位都是具体商品）为例。</p>
<p>　　广告系统是一个三方博弈的生态，要同时兼顾用户体验、广告主利益、平台收入三方的诉求。淘宝广告的计费方式为CPC，即广告主事先设定单次点击出价bid price，每次请求来了广告系统预测用户点击概率pCTR，然后按照bid*pCTR即eCPM排序，分数高的广告获得展示，如果用户点击了广告则广告平台获得广告主的费用bid price（为了简化问题暂先忽略GSP的影响）。</p>
<p>　　按照这种方式主要是以广告平台的收入为优化目标，广告主即便出高价获得了流量但是ROI没法保证，即流量质量没法保证。广告主可以对不同的广告位不同的人群设定不同的出价来缓解，但粒度还是太粗了，如果广告系统能在每次请求这个细粒度上根据pCVR来帮助广告主自动调整出价就完美了，即pCVR高的请求出价高，pCVR低的请求出价低，保证ROI不会降，这就是OCPC的核心思想。</p>
<p>　　淘宝的广告主还有一个特点，他们本身就是淘宝的商家，且大部分是中小商家，GMV是他们的最重要诉求，广告预算一般都是占他们GMV的固定比例，因此提高GMV还能带来广告预算的增长，对广告平台的长期发展也是有利的。GMV提高一定程度上也反映了用户体验的提高，毕竟你推的广告被用户相中了还消费了。</p>
<p>　　至此，算法的框架已经基本成型：以ROI不降为约束，通过算法自动调整出价来尽量优化eCPM和GMV。</p>
<h2 id="u7B97_u6CD5_u7EC6_u8282"><a href="#u7B97_u6CD5_u7EC6_u8282" class="headerlink" title="算法细节"></a>算法细节</h2><h3 id="1-__u5148_u7ED9_u51FA_u4E00_u7CFB_u5217_u5B9A_u4E49"><a href="#1-__u5148_u7ED9_u51FA_u4E00_u7CFB_u5217_u5B9A_u4E49" class="headerlink" title="1. 先给出一系列定义"></a>1. 先给出一系列定义</h3><ul>
<li>定义用户$u$在点击广告$a$之后发生交易转化事件$c$的概率为$p(c|u,a)$，也就是所谓的从点击到购买的转化率CVR。</li>
<li>定义$v_a$为广告商品$a$的pay-per-buy(PPB)，也就是商家的收入，因此单次点击的期望GMV为$p(c|u,a)*v_a$。</li>
<li>用户$u$对广告$a$的单次点击的期望ROI，这里忽略GSP的影响：<br>$$<br>roi_{(u,a)}=\frac{p(c|u,a)*v_a}{b_a}<br>$$</li>
<li>广告$a$在一段时间内总的期望ROI为：<br>$$<br>roi_a=\frac{v_a\cdot\sum_u n_u\cdot p(c|u,a)}{b_a\cdot\sum_u n_u}=\frac{E_u[p(c|u,a)]\cdot v_a}{b_a}<br>$$<br>其中$n_u$为一段时间内用户$u$对广告$a$的点击次数。$E_u[p(c|u,a)]$ 的计算可以通过一段时间内预测模型给出的广告$a$所有pCVR值求均值得到，但要去除最大10%和最小10%的pCVR值，目的应该是去除异常点，使结果更加准确可靠。</li>
</ul>
<h3 id="2-_bid_u4F18_u5316_u7684_u4E0A_u4E0B_u754C"><a href="#2-_bid_u4F18_u5316_u7684_u4E0A_u4E0B_u754C" class="headerlink" title="2. bid优化的上下界"></a>2. bid优化的上下界</h3><p>　　$roi_a$与$E_u[p(c|u,a)]$ 成线性关系，对于单次请求，假设出价从$b_a$调整为$b_a^*$，只要满足<br>$$<br>\frac{b_a^*}{b_a}\le\frac{p(c|u,a)}{E_u[p(c|u,a)]}<br>$$<br>即能保证$roi_a$不降。</p>
<p>　　论文给出的最终上下界如下，其实在其中考虑商业利益做了一定的妥协，并不能保证所有广告的ROI都一定不降，论文后面的实验结果也说明了这一点，大部分广告的ROI都有提高，少部分有降，但总体还是提高了，<br>$$<br>l(b_a^*)=<br>\begin{cases}<br>b_a\cdot(1-r_a), &amp; \frac{p(c|u,a)}{E_u[p(c|u,a)]}&lt;1 \\<br>b_a, &amp; \frac{p(c|u,a)}{E_u[p(c|u,a)]}\ge 1 \\<br>\end{cases} \\<br>u(b_a^*)=<br>\begin{cases}<br>b_a, &amp; \frac{p(c|u,a)}{E_u[p(c|u,a)]}&lt;1 \\<br>b_a\cdot\min(1+r_a,\frac{p(c|u,a)}{E_u[p(c|u,a)]}), &amp; \frac{p(c|u,a)}{E_u[p(c|u,a)]}\ge 1 \\<br>\end{cases}<br>$$<br>其中$l(b_a^*)$ 为下限，$u(b_a^*)$ 为上限，还有个上下浮动的阈值参数$r_a$（比如40%），看下图更加明显，灰色区域为$b_a^*$ 的候选取值范围，即所谓的可行域（feasible region）。可以看到当流量质量较差时，$b_a^*$ 的上限还能到$b_a$ 本身，这显然是不能保证ROI不降的，这里做了妥协。</p>
<p><img src="/img/ocpc_figure3.jpg" alt=""></p>
<h3 id="3-_Ranking"><a href="#3-_Ranking" class="headerlink" title="3. Ranking"></a>3. Ranking</h3><p>　　在依赖eCPM的排序机制下，在可行域内选取不同的$b_a^*$ 可能会导致排序结果的不同，进而影响到其他的指标。</p>
<p>　　先看最简单的情况，只有1个广告位，候选广告集合A中共有n个广告，通过调整出价来竞争这个广告位，优化问题的数学形式如下：<br>$$<br>\max_{b_1^*,\cdots,b_n^*}f(k,b_k^*)\qquad\qquad\qquad\qquad \\<br>s.t.\qquad k=\mathop{\arg\max}_i\ pctr_i*b_i^* \\<br>\qquad\qquad l(b_i^*)\le b_i^*\le u(b_i^*),\forall i\in A<br>$$<br>　　$k$是最终胜出的广告，依赖于$b_1^*,\cdots,b_n^*$ 的选取，$f(\cdot)$ 是需要优化的目标函数，综合了我们关注的指标。比如下面两个例子：<br>$$<br>f_1(k,b_k^*)=pctr_k*pcvr_k*v_k\qquad\qquad\qquad \\<br>f_2(k,b_k^*)=pctr_k*pcvr_k*v_k+\alpha*pctr_k*b_k^*<br>$$<br>　　$f_1$只考虑GMV，而$f_2$同时考虑GMV和eCPM即广告平台收入。论文后面的实验部分给出了一种更复杂的形式，同样是综合GMV和eCPM：<br>$$<br>f(k,b_k^*)=pctr_k*b_k^**(1+\sigma(\frac{pcvr_k*v_k*||A||}{\sum_{i\in A}pcvr_i*v_i},w)*r_a)<br>$$<br>其中$w=6$，$r_a=0.4$，$\sigma(x,w)=\frac{x^w-1}{x^w+1}$，当$w&gt;0$时，$\sigma(x,w)$ 是一个关于$x$的值域范围(-1,1)的单调增函数。</p>
<p>　　以上所有的$f(k,b_k^*)$ 函数都是$b_k^*$ 的单调增函数（更准确的说应该是单调非减函数，论文里没有严格区分），这为后面的ranking算法提供了便利。一般来说，这个单调增的假设也是合理的，毕竟出价高了对于大部分指标都是好事。<br><br></p>
<ul>
<li>优化问题求解方法：<br>设$s_a^*=pctr_a*b_a^*$，则$s_a^*$ 的下界和上界分别为$l(s_a^*)=pctr_a*l(b_a^*)$，$u(s_a^*)=pctr_a*u(b_a^*)$，将所有的$f(i,u(b_i^*))$ 按降序排列，然后按顺序找出第一个广告$k$，满足$u(s_k^*)$ 大于等于其他所有的$l(s_i^*)$，这个$k$就是最终展示的广告，且$b_k^*=u(b_k^*)$。<br><br></li>
<li>论文省略了求解方法正确性的严格证明，这里补充一下：<br>a. 首先最终结果$b_k^*$ 一定等于$u(b_k^*)$。<br>反证法：假设最终结果为$b_k^*$，且 $b_k^*\lt u(b_k^*)$ 。由约束可知 $pctr_k*b_k^*$ 大于等于其他所有的$pctr_i*b_i^*$，若取新的$b_k^{+}=u(b_k^*)$，则有$pctr_k*b_k^{+}&gt;pctr_k*b_k^*$，$pctr_k*b_k^{+}$ 仍然大于等于其他所有的$pctr_i*b_i^*$，即最终胜出的广告还是$k$，但由$f(\cdot)$ 的单调递增性，$f(k,b_k^{+})&gt;f(k,b_k^*)$，矛盾。<br>b. 由a可知，$f(\cdot)$ 的最大值一定是某个$f(i,u(b_i^*))$，$i$从1到$n$。我们当然希望越大越好，因此从大到小排列挨个检验，$f(k,u(b_k^*))$ 能雀屏中选的前提是$u(b_k^*)$ 实力够硬，能够在其他$b_i^*$ 配合的情况下满足约束条件：$pctr_k*u(b_k^*)$ 大于等于其他所有的$pctr_i*b_i^*$，既然其他$b_i^*$ 很配合，当然都取最小值$l(b_i^*)$ 最好，所以$u(b_k^*)$ 的条件放松到$pctr_k*u(b_k^*)$ 大于等于其他所有的$pctr_i*l(b_i^*)$，也即$u(s_k^*)$ 大于等于其他所有的$l(s_i^*)$。<br>证毕。<br>综上，$f(\cdot)$ 的单调性是算法正确性的保证，复杂度也不高，主要就是个排序。<br><p><br>　　说完1个广告位的情况，再推广到N个广告位，算法如下图，本质上是个贪心算法，先按照1个广告位的算法找出广告$k$放到广告位1，然后调整剩下广告的$u(s_i^*)\le u(s_k^*)$，不然$k$就不能保证排在第1位了，同时调整剩下广告的$u(b_i^*)$，接着在剩下的广告中按1个广告位的算法找出第2个广告位的广告，依次类推：</p></li>
</ul>
<p><img src="/img/ocpc_alg1.jpg" alt=""></p>
<h3 id="4-__u6A21_u578B_u6821_u6B63_uFF08Calibration_uFF09"><a href="#4-__u6A21_u578B_u6821_u6B63_uFF08Calibration_uFF09" class="headerlink" title="4. 模型校正（Calibration）"></a>4. 模型校正（Calibration）</h3><p>　　论文提到他们模型的pCVR有偏差，当真实CVR越大，pCVR和真实CVR的比值越大，也即偏差越大，所以需要校正。论文根据实验观察给出了一个校正的经验公式：<br>$$<br>p(c|u,a)=<br>\begin{cases}<br>p(c|u,a), &amp; p(c|u,a)&lt;tc \\<br>p(c|u,a)*(1+\log(\frac{p(c|u,a)}{tc})), &amp; p(c|u,a)\ge tc \\<br>\end{cases}<br>$$<br>$tc$为阈值，取0.012。我们之前的做法是采用保序回归，不知道孰优孰劣。</p>
<h3 id="5-__u6A21_u578B_u8BC4_u4F30"><a href="#5-__u6A21_u578B_u8BC4_u4F30" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h3><p>　　CTR/CVR模型线下评估一般都用AUC，论文提到Google的那篇Wide &amp; Deep Learning for Recommender Systems里表明线下AUC高上线可能效果反而变差。Wide &amp; Deep那篇论文我之前看过，是说他们的纯Deep模型相比纯Wide模型（即LR）线下AUC降了，但线上效果反而提升了，Google作者也没给出很好的解释。</p>
<p>　　这篇阿里的论文也说他们碰到了类似的现象，于是想出一个新的指标Group AUC (GAUC)，即将测试数据按照用户u和广告位p的组合(u,p)分组计算AUC（如果某个组全是正样本或全是负样本，则忽略这个组），最后再按权重求平均，权重可以是各组的展示次数或点击次数。具体公式如下：<br>$$<br>GAUC=\frac{\sum_{(u,p)}w_{(u,p)}*AUC_{(u,p)}}{\sum_{(u,p)}w_{(u,p)}}<br>$$<br>　　可惜论文并没详细说这么做到底能带来多少好处，是否真的解决了前面AUC线下和线上不一致的弊端。</p>
<h2 id="u5B9E_u9A8C_u7ED3_u679C"><a href="#u5B9E_u9A8C_u7ED3_u679C" class="headerlink" title="实验结果"></a>实验结果</h2><p>　　论文分线下模拟和线上效果做了很多角度的实验对比，这里只记录几个主要的部分。</p>
<h3 id="1-__u7EBF_u4E0B_u6A21_u62DF"><a href="#1-__u7EBF_u4E0B_u6A21_u62DF" class="headerlink" title="1. 线下模拟"></a>1. 线下模拟</h3><p>　　通过历史的log数据，将pCTR和pCVR当做真实的CTR和CVR，比如某次展示的广告计算出pCTR为4%，则认为贡献了0.04的点击。然后设计各种策略，统计关心的指标。</p>
<p>一共有4种策略：</p>
<ul>
<li>Strategy 0为对照组，保持原来线上的状态</li>
<li>Strategy 1站在广告主角度，设定一个简单的调价格规则$b_a^*=b_a*(1+\sigma(\frac{p(c|u,a)}{E_u[p(c|u,a)]},w)*r_a)$</li>
<li>Strategy 2就是OCPC策略，优化的目标函数前面说过，是$f(k,b_k^*)=pctr_k*b_k^**(1+\sigma(\frac{pcvr_k*v_k*||A||}{\sum_{i\in A}pcvr_i*v_i},w)*r_a)$</li>
<li>Strategy 3不调出价而是直接修改rankscore公式，不再是eCPM排序，而改成按$pctr*pcvr*bid$，很明显是想提升GMV</li>
</ul>
<p>　　实验结果如下图，相对Strategy 0，Strategy 1和3的GPM（即千次展示GMV）和ROI都提高了但RPM降了，只有Strategy 2即OCPC策略在3个指标上都获得了提升，达到三方共赢。</p>
<p><img src="/img/ocpc_table1.jpg" alt=""></p>
<h3 id="2-__u7EBF_u4E0A_u6548_u679C"><a href="#2-__u7EBF_u4E0A_u6548_u679C" class="headerlink" title="2. 线上效果"></a>2. 线上效果</h3><p>　　Strategy 2相比Strategy 0和线下模拟一致，依然在三个指标上都获得了提升，见下图：</p>
<p><img src="/img/ocpc_table4.jpg" alt=""></p>
<p>　　论文始终强调，这套机制具有普适性，并不局限于GMV，论文给出一个例子，双十一前淘宝商家更加关注商品加入购物篮的量，于是使用模型预测的商品加入购物篮概率pASR，修改目标函数为：<br>$f(k,b_k^*)=pctr_k*b_k^**(1+\sigma(\frac{pasr_k*||A||}{\sum_{i\in A}pasr_i},w)*r_a)$，<br>相比旧的公式，ASR指标提升15.6%。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>　　这又是一篇从实践中来的论文，针对真实场景的问题建模，算法原理也不是多么的高深复杂，却取得了很好的效果。阿里在广告主数据方面得天独厚，广告主本身就是淘宝商家，ROI的数据可以轻易获得。如果是别的广告平台想做ROI优化可能会困难重重，首先各家广告主的ROI诉求可能千差万别，其次数据共享更是难如登天。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/10/21/ocpc_roi/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/10/08/explore_exploit/">Paper Reading: Explore and Exploit</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-10-08T10:18:23.000Z">Oct 8 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="u8BBA_u6587_u5B66_u4E60_uFF1AExplore_and_Exploit"><a href="#u8BBA_u6587_u5B66_u4E60_uFF1AExplore_and_Exploit" class="headerlink" title="论文学习：Explore and Exploit"></a>论文学习：Explore and Exploit</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　前阵子读了一篇KDD2017的论文”A Practical Exploration System for Search Advertising”，是有关E&amp;E算法的，放假无事便调研了一些相关资料，做个小结。</p>
<p>　　计算广告和推荐系统中经常会碰到某些长尾广告或长尾item从来没有机会或者很少机会的展示，导致CTR预估非常不准，需要探索性地创造机会给它们一定的展示量，但又不能带来太大的损失。这种问题一般称作Explore and Exploit问题。</p>
<p>　　在学术界经常把它描述成为一个多臂赌博机问题(multi-armed bandit problem, MAB)，若干台赌博机，每次可以选择一台机器摇一下，有一定概率会吐钱出来，但各台机器吐钱概率不一致且未知，那么赌徒每次该如何选择来最大化收益？对于K-armed bandit problem数学定义为：</p>
<p>　　$i$ 为机器下标，$1\le i\le K$ ，每台机器对应一组随机变量$X_{i,1},X_{i,2},X_{i,3}…$，表示每次被选中后的收益，这组随机变量独立同分布，期望 $\mu_i$未知。</p>
<p>　　策略A是一个算法，每次选择一台机器获得其收益，定义$T_i(n)$ 为前n次选择中机器$i$被选中的次数，定义策略A的前n次选择的regret为：<br>$$<br>\mu^*n-\mu_j\sum_{j=1}^{K}E[T_j(n)]<br>$$<br>　　其中<br>$$<br>\mu^*\mathop{=}^{def}\max_{1\le i\le K}\mu_i<br>$$<br>　　regret越小表示策略A越好，看起来有点像online learning的regret定义。下面总结几种最近调研的E&amp;E算法。</p>
<h2 id="u4E00_uFF1A_u7EAF_u968F_u673A"><a href="#u4E00_uFF1A_u7EAF_u968F_u673A" class="headerlink" title="一：纯随机"></a>一：纯随机</h2><p>　　最简单的办法，每次随机选。</p>
<h2 id="u4E8C_uFF1A_u6700_u5927_u5747_u503C"><a href="#u4E8C_uFF1A_u6700_u5927_u5747_u503C" class="headerlink" title="二：最大均值"></a>二：最大均值</h2><p>　　先随机选若干次，然后一直选均值最大的。</p>
<h2 id="u4E09_uFF1A_24_5Cepsilon_24-Greedy"><a href="#u4E09_uFF1A_24_5Cepsilon_24-Greedy" class="headerlink" title="三：$\epsilon$-Greedy"></a>三：$\epsilon$-Greedy</h2><p>　　设定参数 $\epsilon\in(0,1]$，每次以 $\epsilon$ 的概率随机选一个机器，否则选择当前收益均值最大的机器。</p>
<p>　　论文[1]中给出了一个变种算法：$\epsilon_n$-Greedy，具体为 $\epsilon$ 不再固定，而是以1/n的速率衰减，可以证明比原始方法有更好的regret。具体见论文[1]的Theorem 3。</p>
<h2 id="u56DB_uFF1AThompson_Sampling"><a href="#u56DB_uFF1AThompson_Sampling" class="headerlink" title="四：Thompson Sampling"></a>四：Thompson Sampling</h2><p>　　对于收益为1和0二值的情况，可以假定每台机器$i$收益符合参数为$p_i$的伯努利分布，假定参数 $p_i\sim Beta(\alpha_i,\beta_i)$，（搞过LDA的应该都知道Beta分布和伯努利分布的共轭关系）。</p>
<p>　　算法具体为：每一轮每台机器用其当前自身的Beta分布生成一个数$p$，本轮选择$p$最大的那台机器，假定下标为$i$；然后观察其收益，如果为1则 $\alpha_i$加1，为0则 $\beta_i$加1。</p>
<h2 id="u4E94_uFF1AUCB_28Upper_Confidence_Bound_29"><a href="#u4E94_uFF1AUCB_28Upper_Confidence_Bound_29" class="headerlink" title="五：UCB(Upper Confidence Bound)"></a>五：UCB(Upper Confidence Bound)</h2><p>　　论文[1]中不止一个UCB，这里只介绍最简单的UCB1。</p>
<p>　　1. 初始化：每台机器选择一次</p>
<p>　　2. 循环：选择机器$i$，满足<br>$$<br>i=\mathop{\arg\max}_{1\le j\le K}(\bar x_j+\sqrt{\frac{2\ln n}{n_j}})<br>$$<br>　　其中，$\bar x_j$是机器$j$的平均收益，$n_j$是机器$j$被选中的次数，$n$为总的轮次数。</p>
<p>　　算法思想很明白，历史均值加上一个置信区间来估计本轮的收益，历史被选中的次数越少则置信区间越大，会加大被选中的机会。</p>
<p>　　可以证明，UCB的regret量级为O(log n)，具体证明过程见论文[1]，证明过程比较复杂，我看了很久才看明白，会用到Chernoff-Hoeffding bound不等式和1/n^2的无穷级数求和。</p>
<h2 id="u516D_uFF1ALinUCB"><a href="#u516D_uFF1ALinUCB" class="headerlink" title="六：LinUCB"></a>六：LinUCB</h2><p>　　LinUCB算法是雅虎2010年提出的，用于新闻推荐，见论文[2]。考虑到前面几种算法都过于简单，根本没有考虑到个性化的问题，不同的user对于同样的item的期望收益（比如CTR）也是不一样的，且item本身也可能随时间动态变化，因此论文[2]重新定义了考虑上下文的MAB问题，在第t轮：</p>
<p>　　1. 算法A观察到当前用户$u_t$和当前的item候选集$A_t$，对于每一个$a\in A_t$和当前$u_t$，有一个特征向量$x_{t,a}$，即代表上下文</p>
<p>　　2. 通过前t-1轮的收益结果，算法A选择一个$a_t\in A_t$，然后得到收益$r_{t,a_t}$，这里 $r_{t,a_t}$ 的期望与$u_t$和$a_t$都有关</p>
<p>　　3. 根据新的观测值 $(x_{t,a_t},a_t,r_{t,a_t})$，算法A更新选择策略</p>
<p></p><p><br>　　论文[2]中给出两种LinUCB，我们这里只说相对简单的第一种。</p>
<p>　　LinUCB假定$r_{t,a}$ 的期望值和 $x_{t,a}$ 成线性关系，这也是LinUCB名字的来历，具体为：<br>$$<br>E[r_{t,a}|x_{t,a}]=x_{t,a}^T\theta_a^*<br>$$<br>　　可以看到每一个item $a$对应一个未知参数 $\theta_a^*$，可以通过历史数据来估计。假定在第t轮的时候，$a$被选中过m次，对应的特征向量和收益构成训练数据 $(D_a,c_a)$，$D_a$为$m\times d$矩阵，即m个特征向量，$c_a$为$m\times 1$向量，即m个收益。</p>
<p>　　通过岭回归可以得到 $\theta_a^*$ 的估计值：<br>$$<br>\hat\theta_a=(D_a^TD_a+I_d)^{-1}D_a^Tc_a<br>$$<br><br></p>
<hr style="height:1px;border:none;border-top:1px dashed #0066CC;">

<h3 id="u5907_u6CE8-_u5CAD_u56DE_u5F52_u7684_u63A8_u5BFC_uFF1A"><a href="#u5907_u6CE8-_u5CAD_u56DE_u5F52_u7684_u63A8_u5BFC_uFF1A" class="headerlink" title="备注-岭回归的推导："></a>备注-岭回归的推导：</h3><p>　　对于线性回归$D\theta=c$，采用最小二乘和L2正则项来估计参数 $\theta$.<br>$$<br>\min_\theta f(\theta)=\min_\theta||c-D\theta||_2^2+\lambda||\theta||_2^2\\<br>f(\theta)=(c-D\theta)^T(c-D\theta)+\lambda\theta^T\theta\\<br>\Rightarrow df=(d(c-D\theta))^T(c-D\theta)+(c-D\theta)^Td(c-D\theta)+\lambda(d\theta)^T\theta+\lambda\theta^Td\theta\\<br>=2(c-D\theta)^Td(c-D\theta)+2\lambda\theta^Td\theta\\<br>=-2(c-D\theta)^TDd\theta+2\lambda\theta^Td\theta<br>$$<br>　　令$df=0$，有<br>$$<br>D^T(c-D\theta)=\lambda\theta\\<br>\Rightarrow D^Tc=(D^TD+\lambda I)\theta\\<br>\Rightarrow \theta=(D^TD+\lambda I)^{-1}D^Tc<br>$$</p>
<p><hr style="height:1px;border:none;border-top:1px dashed #0066CC;"><br><br><br>　　回到正题，得到 $\hat\theta_a$后，有：</p>
<p>　　对于任意 $\delta&gt;0,\alpha=1+\sqrt{\ln(2/\delta)/2}$，<br>$$<br>P\left\{\left|x_{t,a}^T\hat\theta_a-E[r_{t,a}|x_{t,a}]\right|\le\alpha\sqrt{x_{t,a}^T(D_a^TD_a+I_d)^{-1}x_{t,a}}\right\}\ge1-\delta<br>$$<br>　　这部分的推导要用到论文[3]的定理，我还没来得及细看。</p>
<p>　　有了这个置信区间的不等式，就可以类似UCB算法一样，在第t轮选择：<br>$$<br>a_t\mathop{=}^{def}\mathop{\arg\max}_{a\in A_t}\left(x_{t,a}^T\hat\theta_a+\alpha\sqrt{x_{t,a}^T(D_a^TD_a+I_d)^{-1}x_{t,a}}\right)<br>$$<br>　　算法具体如下图：</p>
<p><img src="/img/LinUCB.jpg" alt=""></p>
<p>　　可以看到，算法涉及矩阵求逆等运算，如果特征向量维度很高，计算会很复杂。因此论文[2]介绍了如何构建特征，主要是如何降维。user相关的原始特征1193维，item相关的原始特征83维，user特征用 $\phi_u$ 表示，item特征用 $\phi_a$ 表示。用LR构建点击模型来拟合历史数据，不过线性部分比较特别，形式为 $\phi_u^TW\phi_a$，即考虑了user和item的所有二阶特征组合，通过LR训练出参数矩阵$W$后，将user特征投影到item的特征空间：<br>$$<br>\psi_u\mathop{=}^{def}\phi_u^TW<br>$$<br>　　然后再聚类为5个簇，再加上偏置项，即特征向量 $x_{t,a}$ 一共只有6维，计算将大大简化。</p>
<p>　　论文[2]还有一个有意思的部分，介绍了如何offline来评估E&amp;E算法的好坏。比如我们有一份完全随机策略的log数据，希望能够线下评估我们新的E&amp;E算法A，具体为：</p>
<p>　　1. 依次扫描log数据的每一条记录，如果展示的item和算法A给出的不一致，则丢弃该条记录，直到找到和算法A一致的一条记录</p>
<p>　　2. 该记录加入到算法A的历史，算法A更新选择策略</p>
<p>　　3. 该记录的收益加入到A的总收益，如果A的历史记录不够T条，回到第1步，从上次的位置继续扫描</p>
<p>　　4. 直到A的历史数据达到T条，最后输出总收益除以T</p>
<h2 id="u4E03_uFF1AExploration_Algorithm_for_Search_Ads"><a href="#u4E03_uFF1AExploration_Algorithm_for_Search_Ads" class="headerlink" title="七：Exploration Algorithm for Search Ads"></a>七：Exploration Algorithm for Search Ads</h2><p>　　前面的算法都是把E&amp;E独立出来讲，假如我们的业务已经有了一个click model，那该如何和E&amp;E结合呢？A Practical Exploration System for Search Advertising[4]给出了一个很好的方案。这篇论文来自KDD2017，是雅虎搜索广告团队的工作。</p>
<p>　　冷启动问题特化到计算广告领域，就是指广告主向广告系统提交新的广告，系统很难准确预估新广告的CTR，带来的后果是新的广告可能拿不到多少展示量，click model不能很快学习到这些“冷广告”的点击概率，影响整体效果（比如收入）。</p>
<p>　　仔细分析，如果对新广告的CTR预估高了还好一些，初期就会有足够的展示量，只要模型更新及时，很快学习到更加准确的点击率，然后恢复正常；如果新广告的CTR预估很低，拿不到展示量，模型不能学到准确点击率，即使模型更新，预估还是偏低，恶性循环，导致本来质量不错的新广告永远拿不到量。</p>
<p>　　因此这篇论文的思想便是对CTR预估较低的新广告做boosting，加大预估CTR，具体实现结合了 $\epsilon$-Greedy和UCB的思路。<br><br></p>
<hr style="height:1px;border:none;border-top:1px dashed #0066CC;">

<h3 id="u4E00_u4E9B_u80CC_u666F_u77E5_u8BC6_uFF1A"><a href="#u4E00_u4E9B_u80CC_u666F_u77E5_u8BC6_uFF1A" class="headerlink" title="一些背景知识："></a>一些背景知识：</h3><p>　　搜索广告被Matching模块召回，但拿不到展示量的原因主要有：</p>
<p>　　1. Ad-Quality Filtering(AQF)：广告质量不行被过滤掉，而一般来说预估CTR是质量分的一个重要组成部分，所以预估CTR低了，很可能在这里就被干掉。</p>
<p>　　2. Reserve Price：出于商业利益，广告系统一般会设定一个最低价格的限制，如果bid*pCTR低于这个最低价格，则连参与竞价的机会都没有。</p>
<p>　　3. Auction：最后竞价环节，狼多肉少，广告坑位有限，大家按rank-score排序，分高者胜出。而bid*pCTR又是rank-score的最重要组成部分。</p>
<p>　　显然，这人生的三道坎都与预估CTR直接相关，可见click model何等重要。</p>
<p>　　click model干的事就是给定搜索词q、广告a、用户u之后，预估u对a的点击概率p(click|q,a,u)，一般都采用基于特征的监督学习模型。而最重要的特征往往是所谓的点击反馈特征，即历史点击数和历史展示数以及二者的比值历史CTR，如果考虑了位置偏置，就变成EC/COEC特征。</p>
<p>　　EC/COEC在多个雅虎的论文里提到过，不过最早的出处应该是2007年的这篇[5]。简单来说，搜索广告位有多个，比如竖排一列，不同rank的点击率天然就有差异，一般高处rank的偏高，低处rank的偏低，这就是位置偏置。这种天然点击率可以通过统计每一个rank上的所有点击除以所有展示来得到，当然这是一种近似值，因为广告系统会把rank-sore高的广告往高rank处放，这样统计又引入了rank-score偏置，除非开一部分流量完全随机出广告。</p>
<p>　　我们将每个rank上的天然点击率记为 $ctr(r)$，r为rank编号。比如 $ctr(r_1)$ 是 $ctr(r_2)$ 的两倍，广告A在$r_1$上展示了1000次，点击了100次，广告B在$r_2$上展示了1000次，点击了80次，如果直接将这些数字作为特征的话，模型可能会学出广告A的点击率大于广告B，但其实考虑位置偏置，广告A的点击率小于B才是合理的。</p>
<p>　　为此定义expected clicks(EC)为某个广告a在rank r上的期望点击数：<br>$$<br>ec(r,a)=ctr(r)*i_r(a)<br>$$<br>其中 $i_r(a)$ 为a在rank r上的实际展示数。</p>
<p>　　定义clicks over expected clicks(COEC)如下：<br>$$<br>COEC(a)=\frac{\sum_r c(r,a)}{\sum_r ec(r,a)}<br>$$<br>　　其中 $c(r,a)$ 为a在rank r上的实际点击数。</p>
<p>　　$EC(a)=\sum_r ec(r,a)$ 可以看做是广告a历史展示数的一种normalization，如果非要在绝对值上较真的话可以除以最大的 $ctr(r)$，变成<br>$$<br>EC(a)/\max_r ctr(r)，<br>$$<br>即认为最好位置的展示数1次才算1次，其他位置都要打折。</p>
<p>　　同理，COEC(a)可以看做是对广告a的历史CTR做normalize，去除位置偏置。</p>
<p><hr style="height:1px;border:none;border-top:1px dashed #0066CC;"><br><br><br>　　论文算法具体如下图，该算法应放在click model输出预估CTR之后、auction模块之前：</p>
<p><img src="/img/Exploration_Algorithm_for_Search_Ads.jpg" alt=""></p>
<p>　　1：输入，包括当前搜索词q，用户u，召回的广告列表，相应的pCTR，出价bids，和每个广告的历史展示数（去除了位置偏置）。还贴心地准备了一个黑名单列表，即在黑名单里的新广告就不要指望boosting了，听天由命吧，应该是防止严重影响用户体验的广告得到boosting。</p>
<p>　　2：参数，下面碰到了细说。</p>
<p>　　3：照搬 $\epsilon$-Greedy的思路，只在小流量上做Exploration，论文给出的参数 $\epsilon=0.05$，还是比较保守的。</p>
<p>　　4：设定一个要参与Exploration的广告候选集E，先置为空集。</p>
<p>　　5：构建候选集E，对所有召回的广告，如果pCTR小于阈值 $p_{th}$ 且历史展示数小于阈值 $n_{th}$ 且不在黑名单的，加入E。即只对足够新的、预估CTR偏低的广告做Exploration。论文给出的参数 $n_{th}=500$，$p_{th}$ 和AQF模块的阈值一致，为0.02。</p>
<p>　　6：E集合还是偏大，做不到人人有份，所以要抽签决定最终参与boosting的广告，最多 $r_{max}$ 个，论文给出的 $r_{max}$ 只有2。这里抽签的方法要细讲一下，因为论文是把这个作为一个创新点的。抽签采用轮盘赌的方式而不是纯随机，bid越大，被选中的概率越大。这样做的好处有三，一是选出bid高的，最终在auction环节胜出的几率也大，否则折腾半天都白费功夫了；二是保证整体的price-per-click(PPC)够高，防范Exploration带来收入上的损失；三是给了广告主一个正反馈，广告主经常抱怨我在你们这投了新广告钱出的老高怎么量不见涨啊，这下好了，给钱好办事，你出价高我给你量就多，一定程度上还能刺激收入。</p>
<p>　　7：最重要的boosting，将pCTR变大，仿照的是UCB的思路，即在原来pCTR的基础上加上个置信区间。这里的置信区间给的有些任性，论文似乎想从伯努利分布的标准差推导出来，但很不严谨。不管了，反正还有两个参数要调，$\beta$ 是防止分母为0，$\alpha$ 要好好调调，保证boosting之后的广告能有80%越过AQF那道坎。</p>
<p>　　8：形成最终集合F，经过boosting的广告和剩下的广告合并。这里公式似乎有误，$k\in T\setminus[m]$ 应该是 $k\in [m]\setminus T$。</p>
<p>　　9：把最终集合F送到auction模块。</p>
<p></p><p><br>　　算法评估：</p>
<p>　　1. Learning Rate Metrics：论文定义了一个函数，评估新广告从冷变热的速度，实验证明相比对照组，引入boosting后确实变快了。意料之中，无需细说。</p>
<p>　　2. Business Metrics：这是实打实的指标，相比对照组，RPM提升了1%，CTR和PPC等都有一点提升。</p>
<p>　　3. Good versus Bad Ads：这个E&amp;E算法前面说了，就是要帮助本来质量不错的新广告不要被淹没，实验发现有9.5%的Good Ads被挽救了回来。</p>
<p></p><p><br>　　整篇论文给人的感觉就是很“工程”，没有繁琐的理论推导，而有详细的算法框架，原理也很简单直接，还给出了具体参数，最后再给出实际线上效果让你放心。总之就是对我等工程师很友好，拿来即可用，稍微改改就能用到搜索、推荐等领域。</p>
<p><br></p>
<hr>
<p>参考文献：<br>[1] Finite-time Analysis of the Multiarmed Bandit Problem, 2002<br>[2] A Contextual-Bandit Approach to Personalized News Article Recommendation, 2010<br>[3] Exploring compact reinforcement-learning representations with linear regression, 2009<br>[4] A Practical Exploration System for Search Advertising, 2017<br>[5] Comparing Click Logs and Editorial Labels for Training Query Rewriting, 2007</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/10/08/explore_exploit/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/07/16/lambdafm/">lambdaFM</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-07-16T04:28:39.000Z">Jul 16 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="lambdaFM"><a href="#lambdaFM" class="headerlink" title="lambdaFM"></a>lambdaFM</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　前阵子github上有人问我能不能实现一下基于FM的排序模型，于是周末就在<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">alphaFM</a>基础上修改一番，类似lambdaMART的思路，把lambdaRank和FM结合，实现了lambdaFM。</p>
<p>　　代码地址以及使用方法在：<br>　　<a href="https://github.com/CastellanZhang/lambdaFM" target="_blank" rel="external">https://github.com/CastellanZhang/lambdaFM</a></p>
<p>　　同样是FTRL的online learning优化方法，支持高维稀疏特征，单机多线程版本。</p>
<p>　　lambdaFM同时也实现了pairwise的算法，即不考虑deltaNDCG，可以通过参数-rank来选择使用lambdaRank还是pairwise。</p>
<h2 id="pairwise"><a href="#pairwise" class="headerlink" title="pairwise"></a>pairwise</h2><p>　　让我们从pairwise说起，以搜索为例，对于一对样本 $\langle x^i,x^j\rangle$ ，如果 $x^i$ 的相关性好于 $x^j$，我们记为 $x^i\triangleright x^j$ ,反之为 $x^i\triangleleft x^j$ ，相应目标值 $y_{ij}$ 为1或0。建立概率模型如下：<br>$$<br>P_{ij}=P(x^i\triangleright x^j|\Theta)=P(y_{ij}=1|\langle x^i,x^j\rangle,\Theta)\\<br>=\sigma(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))=\frac{1}{1+e^{-(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}}<br>$$<br>　　其中 $\hat{y}(x|\Theta)$ 就是FM的输出：<br>$$<br>\hat{y}(x|\Theta):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\langle v_i,v_j\rangle x_ix_j\\<br>=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nx_ix_j\sum_{f=1}^kv_{i,f}v_{j,f}\\<br>=w_0+\sum_{i=1}^nw_ix_i+\frac{1}{2}\sum_{f=1}^k\left(\left(\sum_{i=1}^nv_{i,f}x_i\right)^2-\sum_{i=1}^nv_{i,f}^2x_i^2\right)<br>$$<br>　　模型参数估计仍然用最大似然，对于所有训练样本对集合 $S$ ，最优化问题为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(\langle x^i,x^j\rangle,y_{ij})\in S}P_{ij}^{y_{ij}}(1-P_{ij})^{1-y_{ij}}<br>$$<br>　　对于训练样本集合 $S$ ，我们总可以调整每一对样本的顺序使得总是有 $x^i\triangleright x^j$ ，即所有的 $y_{ij}$ 都等于1，上面公式简化为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(\langle x^i,x^j\rangle,1)\in S}P_{ij}=\mathop{\arg\min}_{\Theta}\sum_{(\langle x^i,x^j\rangle,1)\in S}-\ln P_{ij}<br>$$<br>　　这样每一对样本 $\langle x^i,x^j\rangle$ 的损失函数为：<br>$$<br>l(\Theta|\langle x^i,x^j\rangle)=-\ln P_{ij}=\ln (1+e^{-(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))})<br>$$<br>　　损失函数对参数求偏导数：<br>$$<br>\frac{\partial l}{\partial\theta}=<br>\frac{\partial l}{\partial(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}(\frac{\partial\hat{y}(x^i|\Theta)}{\partial\theta}-\frac{\partial\hat{y}(x^j|\Theta)}{\partial\theta})<br>$$<br>　　我们令：<br>$$<br>\lambda_{ij}=\frac{\partial l}{\partial(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}=-\frac{1}{1+e^{\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta)}}<br>$$<br>　　则上面公式可以简化为：<br>$$<br>\frac{\partial l}{\partial\theta}=<br>\lambda_{ij}(\frac{\partial\hat{y}(x^i|\Theta)}{\partial\theta}-\frac{\partial\hat{y}(x^j|\Theta)}{\partial\theta})<br>$$<br>　　而FM的输出对参数的偏导数在alphaFM的介绍中已经给出过，下面直接列出。注意，由于是pair的方法，偏置项 $w_0$ 相减总会抵消掉，我们可以固定 $w_0$ 为0，不需要再求解，也就不需要对 $w_0$ 再算偏导数：<br>$$<br>\frac{\partial\hat{y}}{\partial\theta}=<br>\begin{cases}<br>x_i, &amp; if\,\,\theta\,\,is\,\,w_i \\<br>x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 &amp; if\,\,\theta\,\,is\,\,v_{i,f} \\<br>\end{cases}<br>$$<br>　　有了损失函数对参数的偏导，后面优化就是水到渠成。</p>
<h2 id="lambdaRank"><a href="#lambdaRank" class="headerlink" title="lambdaRank"></a>lambdaRank</h2><p>　　在上面的pairwise方法中，可以看到对于每一对样本都是同等对待的，算法尽量使得每一对样本的label大小关系都能预测对，而对于它们具体的label是多少以及在展现中的位置并不敏感。但在实际问题中，当评价指标是NDCG等时，这些信息就很重要了。举个例子：比如同一query下召回了4个doc，实际相关性分数分别为0 1 3 4，我们有两个排序模型A和B，通过模型的打分，排列结果分别为4 3 0 1和3 4 1 0。从pairwise的角度来看，两个排列跟最优排列4 3 1 0都只相差一次交换，似乎模型效果没差别，但是从NDCG的角度来看，NDCG(A) = 0.997，NDCG(B) = 0.852，明显模型A的效果更好。</p>
<p>　　为了迎合NDCG指标，需要在训练的时候对样本pair区别对待，设置不同的权重，lambdaRank提出了一种权重deltaNDCG，即在原来的顺序上如果交换样本 $i$ 和样本 $j$ ，带来的NDCG值变化的绝对值。形式上是将上面公式的 $\lambda_{ij}$ 乘以权重系数，变成 $\lambda_{ij}’$ ：<br>$$<br>\lambda_{ij}’=\lambda_{ij}|\Delta NDCG_{ij}|<br>$$<br>　　这应该就是lambda名字的来源。在我看来，lambdaRank其实仍然是一种pairwise的方法，因为并没有直接对list做优化，跟ListNet等listwise方法有本质不同。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/07/16/lambdafm/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/06/01/mlr_plm/">MLR, PLM</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-06-01T15:25:17.000Z">Jun 1 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="MLR_2C_PLM"><a href="#MLR_2C_PLM" class="headerlink" title="MLR, PLM"></a>MLR, PLM</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　最近阿里的盖坤大神放出了一篇论文Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction，介绍了阿里广告的一个主要ctr预估模型Large Scale Piece-wise Linear Model (LS-PLM)，在2012年就开始使用，据说早期叫做Mixture of LR(MLR)。</p>
<p>　　看完论文就很想验证一下效果，于是基于原来的<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">alphaFM</a>代码很快就实现了一个单机多线程版本，优化算法用了FTRL。完成代码的时候正好又赶上alphaGo完虐人类，于是命名仍然冠以alpha，叫做alphaPLM。</p>
<p>　　代码地址在：<br>　　<a href="https://github.com/CastellanZhang/alphaPLM" target="_blank" rel="external">https://github.com/CastellanZhang/alphaPLM</a></p>
<h2 id="u7B97_u6CD5_u539F_u7406"><a href="#u7B97_u6CD5_u539F_u7406" class="headerlink" title="算法原理"></a>算法原理</h2><p>　　PLM可以看做是混合了聚类和分类的思想，即将特征空间分片或者说分区间，每个分片就是一个聚类，每个聚类对应一个单独的线性模型LR。这里的聚类是软聚类，即每个样本可以属于多个分片，有概率分布。最后计算ctr是先算出在每个分片的ctr，再按属于各个分片的概率加权平均。通过分片线性拟合，达到了非线性的效果。</p>
<p>　　具体模型公式如下：</p>
<p>$$<br>p(y=1|x)=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\frac{1}{1+e^{-w_i^Tx}}\\<br>=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\sigma(w_i^Tx)<br>$$</p>
<p>　　可以看到，聚类部分是用了softmax函数，分类部分就是LR的sigmoid函数。该算法的巧妙之处就是将二者合成一个公式，一起训练参数。公式中m是分片数，属于超参数，由人工给定。模型参数是 $\Theta=\{u_1,…,u_m,w_1,…,w_m\}\in R^{d\times 2m}$，需要训练得到。</p>
<p>　　论文中的优化方法采用的是LBFGS，实现起来比较复杂，我为了快速验证算法效果，优化改成了FTRL，需要计算损失函数对u和w的梯度，下面给出推导。</p>
<p>　　首先对于 $y\in\{-1,1\}$，模型可以统一形式：</p>
<p>$$<br>p(y|x)=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\frac{1}{1+e^{-yw_i^Tx}}\\<br>=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\sigma(yw_i^Tx)<br>$$</p>
<p>　　单条样本(x,y)的损失函数：<br>$$<br>l(\Theta|x,y)=-\ln P(y|x,\Theta)=-\ln\frac{1}{\sum_{j=1}^me^{u_j^Tx}}\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)\\<br>=\ln\sum_{j=1}^me^{u_j^Tx}-\ln(\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx))<br>$$<br>　　梯度计算：<br>$$<br>\nabla_{u_k}l=\frac{e^{u_k^Tx}x}{\sum_{j=1}^me^{u_j^Tx}}-\frac{e^{u_k^Tx}\sigma(yw_k^Tx)x}{\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)}<br>$$<br>$$<br>\nabla_{w_k}l=\frac{ye^{u_k^Tx}\sigma(yw_k^Tx)(\sigma(yw_k^Tx)-1)x}{\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)}<br>$$<br>　　后续的FTRL算法框架和alphaFM非常类似，不再详述，可以参见之前的<a href="http://castellanzhang.github.io/2016/10/16/fm_ftrl_softmax/" target="_blank" rel="external">文档</a>和实现代码。</p>
<h2 id="u7B97_u6CD5_u6548_u679C"><a href="#u7B97_u6CD5_u6548_u679C" class="headerlink" title="算法效果"></a>算法效果</h2><p>　　论文中给了一个demo数据的例子，如下图：</p>
<p><img src="/img/plm.jpg" alt=""></p>
<p>　　我们可以通过代码生成类似的样本，来验证一下算法效果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#demo_data1.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">n = int(sys.argv[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    x = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    y = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    label = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> abs(x) + abs(y) &lt; <span class="number">1.0</span>:</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">    <span class="keyword">print</span> str(label) + <span class="string">" x:"</span> + str(x) + <span class="string">" y:"</span> + str(y)</span><br></pre></td></tr></table></figure></p>
<p>　　生成1万条训练样本和2000条测试样本：</p>
<p>　　<code>python demo_data1.py 10000 &gt; train.txt</code></p>
<p>　　<code>python demo_data1.py 2000 &gt; test.txt</code></p>
<p>　　alphaPLM的训练参数如下：</p>
<p>　　<code>cat train.txt | ./plm_train -m model.txt -u_bias 1 -w_bias 1 -u_l1 0.001 -u_l2 0.1 -w_l1 0.001 -w_l2 0.1 -core 1 -piece_num 4 -u_stdev 1 -w_stdev 1 -u_alpha 10 -w_alpha 10</code></p>
<p>　　在测试集的AUC可以达到0.99以上。</p>
<p>　　如果是LR或FM，你会发现无论你怎么调参，AUC始终在0.5左右。</p>
<p>　　直观上也很容易理解，看图就会发现，如果特征就是x和y的坐标值的话，数据非线性可分，而很明显在四个象限的分片里分别都是线性可分的。</p>
<p>　　你心里是否已经忍不住开始唾弃LR：“啊呸，LR果然是个战五渣！连这么个demo数据都搞不定！”那你可就冤枉LR了，原始特征非线性，完全可以通过转换变成线性或近似线性，比如做个简单的离散化就会大不一样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#demo_data2.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">n = int(sys.argv[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    x = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    y = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    label = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> abs(x) + abs(y) &lt; <span class="number">1.0</span>:</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">    fx = <span class="string">"x"</span> + str(int(x*<span class="number">10</span>)) + <span class="string">":1"</span></span><br><span class="line">    fy = <span class="string">"y"</span> + str(int(y*<span class="number">10</span>)) + <span class="string">":1"</span></span><br><span class="line">    <span class="keyword">print</span> str(label) + <span class="string">" "</span> + fx + <span class="string">" "</span> + fy</span><br></pre></td></tr></table></figure></p>
<p>　　重新生成1万条训练样本和2000条测试样本：</p>
<p>　　<code>python demo_data2.py 10000 &gt; train.txt</code></p>
<p>　　<code>python demo_data2.py 2000 &gt; test.txt</code></p>
<p>　　此时你会发现，无论LR、FM还是PLM，AUC都很容易达到0.99以上。</p>
<p>　　而在我们广告业务的真实数据中，LR所面对的几乎都是千万维或上亿维的高维离散特征，效果并不会比FM或PLM差太多。我在真实数据的实验也验证了这一点，PLM和FM相比LR提升的都差不多，AUC基本都是在千分位提高几个点。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/06/01/mlr_plm/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/11/22/ensembling_lagrange/">Ensembling, Lagrange</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-11-22T07:31:26.000Z">Nov 22 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="Ensembling_2C_Lagrange"><a href="#Ensembling_2C_Lagrange" class="headerlink" title="Ensembling, Lagrange"></a>Ensembling, Lagrange</h1><p>　　据我观察，名字带“拉”的人一般都很厉害，比如马拉多纳、希拉里、狄波拉、张娜拉、杜拉拉、陈法拉、尼古拉斯·赵四、地铁站的罗拉……</p>
<p>　　但跟我们今天的大神拉格朗日（Lagrange）一比就都被秒成了渣渣，因为每一个上过高数和物理的我们都被拉大神无情碾压摩擦过啊~</p>
<p>　　童鞋，请站起来回答拉格朗日中值定律是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日乘子法是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日内插公式是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日点是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日函数是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日方程是啥？</p>
<p>　　……</p>
<p>　　童鞋，你肿么站不起来了童鞋？<br><br><br>　　请原谅我召唤出封印在你们心底多年的梦魇，快跟我一起踏碎时空大逃亡，继续闪回到小米数据挖掘大赛那几天。</p>
<p>　　<a href="http://castellanzhang.github.io/2016/10/16/fm_ftrl_softmax/" target="_blank" rel="external">上文</a>中已经提到我们最终的模型不止一个，而是多个DNN和FM共七八个模型的融合。</p>
<p>　　所谓模型融合，洋气一点叫Ensembling，在 <a href="http://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">http://mlwave.com/kaggle-ensembling-guide/</a> 一文中有详细介绍。简单来说就是综合多个单模型的输出来给出最终的结果，一般会比每个单模型的效果都好，现在已经是各大比赛的常规武器。但对于初涉江湖的小伙伴们还没有太多经验，一开始只是简单的平均，后来尝试了各个模型的输出做为LR的特征，发现没啥卵用，比赛已近尾声就没再折腾更复杂的方法，最终使用线性加权平均，即：<br>$$<br>\hat y=w_1\hat y_1+w_2\hat y_2+…+w_M\hat y_M<br>$$<br>　　其中，$w_1+w_2+…+w_M=1$</p>
<p>　　这里有个问题就是权重系数怎么定？</p>
<p>　　一开始就是拍脑袋定，单模型效果好的权重就大一点，效果差的就小一点。后来小伙伴使用暴力网格搜索的方法，融合两三个模型还行，再多就已经慢到不可接受。</p>
<p>　　我看在眼里急在心头，我们是模武双修的种族啊，怎么能只用暴力解决呢？这种目标如此鲜明灿若煌煌皓月的好问题，当然要祭出流光华丽的大模型才相得益彰呢！</p>
<h2 id="u7B2C_u4E00_u5F0F_uFF1A_u6DF7_u6C8C_u521D_u5206_u6A21_u578B_u73B0_uFF01"><a href="#u7B2C_u4E00_u5F0F_uFF1A_u6DF7_u6C8C_u521D_u5206_u6A21_u578B_u73B0_uFF01" class="headerlink" title="第一式：混沌初分模型现！"></a>第一式：混沌初分模型现！</h2><p>　　比如我们有M个单模型分类器，解决K分类问题，测试集包含N条样本，$\hat{y}_{nmk}$ 表示第m个单模型对第n条样本属于第k类的预测概率。</p>
<p>　　我们采用线性加权平均的方法，融合M个模型得到的预测概率<br>$$<br>\hat{y}_{nk}=\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk}<br>$$<br>　　比赛评价标准为logloss，我们希望融合后的模型在测试集上的logloss尽量小，所以优化目标如下：<br>$$<br>\min_{w}f(w)=\min_{w}\{-\frac{1}{N}\sum_{n=1}^{N}\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})\}<br>$$<br>　　其中$w=[w_1,..,w_{M-1}]^T$是需要求解的权重参数，我们知道权重之和要归一，所以不需要$w_M$这一变量，用$1-\sum_{m=1}^{M-1}w_m$代替即可。</p>
<p>　　问题已明确，下面来求解。</p>
<h2 id="u7B2C_u4E8C_u5F0F_uFF1A_u68AF_u5EA6_u6740_uFF01_uFF01"><a href="#u7B2C_u4E8C_u5F0F_uFF1A_u68AF_u5EA6_u6740_uFF01_uFF01" class="headerlink" title="第二式：梯度杀！！"></a>第二式：梯度杀！！</h2><p>　　这一招早已驾轻就熟，直接求梯度，然后上sgd或adagrad。单条样本的损失函数为<br>$$<br>l_n(w)=-\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})<br>$$<br>　　梯度为<br>$$<br>\frac{\partial l_n}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}-\hat{y}_{nMk}}{\hat{y}_{nk}},\quad m=1,…,M-1<br>$$<br>　　adagrad版本的代码如下，注意其中变量名和上面的公式并不完全一致，类别编号是从0到K-1（我就是这么洒脱随性~）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">n = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    g = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum-<span class="number">1</span>):</span><br><span class="line">        g[m] = (x[modelNum-<span class="number">1</span>][label]-x[m][label])/pre[label]</span><br><span class="line">        n[m] += g[m] * g[m]</span><br><span class="line">        lr = alfa/(beta+math.sqrt(n[m]))</span><br><span class="line">        w[m] -= lr * g[m]</span><br><span class="line">        wsum += w[m]</span><br><span class="line">    w[modelNum-<span class="number">1</span>] = <span class="number">1</span>-wsum</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure></p>
<p>　　输入数据的格式如下：</p>
<p><code>label score11,score12,...,score1K score21,score22,...,score2K ... scoreM1,scoreM2,...,scoreMK</code></p>
<p>　　举个例子，比如二分类4个模型的数据片段如下：</p>
<pre><code>0 0.912427,0.0875725 0.905673,0.0943273 0.911398,0.088602 0.933166,0.066834
0 0.86061,0.139389 0.865925,0.134075 0.881552,0.118448 0.872196,0.127804
1 0.364299,0.635701 0.32974,0.67026 0.323839,0.676161 0.341047,0.658953
0 0.715563,0.284437 0.713995,0.286005 0.713599,0.286401 0.713383,0.286617
0 0.948186,0.0518137 0.925587,0.0744127 0.929451,0.0705489 0.939952,0.0600485
0 0.58531,0.41469 0.588321,0.411679 0.520456,0.479544 0.671846,0.328154
1 0.0154455,0.984554 0.0150741,0.984926 0.0118866,0.988113 0.0109413,0.989059
1 0.0472074,0.952793 0.0612501,0.93875 0.065446,0.934554 0.061947,0.938053
1 0.430228,0.569772 0.366636,0.633364 0.337206,0.662794 0.5487,0.4513
0 0.97958,0.0204195 0.980348,0.0196517 0.979461,0.0205394 0.985608,0.0143915
</code></pre><p>　　我们的测试数据共200多万条，时间复杂度跟单模型数M呈线性关系，跑一次也就分分钟的事。为了验证算法，做了对比，对于3模型融合跑出的结果和暴力求解的结果一致。</p>
<p>　　问题似乎就这么完美解决了，但人生就像直流充电桩，上一秒还是70A的电流，下一秒就可能跳电。当我们融合更多模型时问题出现了，有的权重算出了负值！就像下面这样：</p>
<pre><code>0.120181463777    gender_cut_ftrl8000_val
0.0929962960391   gender_cut_ftrl_val
0.0245135332374   gender_nn_1005_val
0.269705618425    gender_cut_ftrl2000_val
0.422565675064    gender_cut_ftrl1000_val
0.233316720486    gender_newFM_val
0.0228310140994   gender_xxFM_val
-0.186110321128   gender_result_fm
</code></pre><p>导致最终模型的预测概率有可能大于1或小于0，这如何能忍，必须再创新招！</p>
<h2 id="u7B2C_u4E09_u5F0F_uFF1A_u751F_u6B7B_u7B26_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E09_u5F0F_uFF1A_u751F_u6B7B_u7B26_uFF01_uFF01_uFF01" class="headerlink" title="第三式：生死符！！！"></a>第三式：生死符！！！</h2><p>　　“这生死符一发作，一日厉害一日，奇痒剧痛递加九九八十一日，然后逐步减退，八十一日之后，又再递增，如此周而复始，永无休止。每年我派人巡行各洞各岛，赐以镇痛止痒之药，这生死符一年之内便可不发。”</p>
<p>　　好啦，不吓你啦，这是金庸老先生笔下天山童姥的生死符。我这里的生死符简单得很，顾名思义，由权重符号决定单模型生死，正的留，负的弃，留下的单模型重新计算权重，有可能又有新的负值出现，再次使用生死符，直到剩下的单模型权重全部大于等于0。</p>
<p>　　还以上面那次融合为例，最终把gender_xxFM_val和gender_result_fm都干掉了，剩下的权重为：</p>
<pre><code>0.122788741689    gender_cut_ftrl8000_val
0.0782434412719   gender_cut_ftrl_val
0.0331138038226   gender_nn_1005_val
0.24358375583     gender_cut_ftrl2000_val
0.446465338463    gender_cut_ftrl1000_val
0.0758049189234   gender_newFM_val
</code></pre><p>　　在测试集上的logloss = 0.435116，而其中单模型最好的logloss是0.436593。</p>
<p>　　相应地，这批融合在7分类问题age上效果更加明显，从最好的单模型1.35416降到1.35061。<br><br><br>　　以上便是比赛期间我们使用的全部招式。</p>
<p><br><br><br><br><br><br><br>　　憋走，故事还没结束，忘了塞纳河畔的拉格朗日了吗？</p>
<p>　　像我这等追求卓越的好青年岂能够留下不完美的话柄，虽然比赛结束了，我还是要把它彻底解决掉——快使出</p>
<h2 id="u7B2C_u56DB_u5F0F_uFF1A_u94C1_u9501_u6A2A_u6C5F_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u56DB_u5F0F_uFF1A_u94C1_u9501_u6A2A_u6C5F_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第四式：铁锁横江！！！！"></a>第四式：铁锁横江！！！！</h2><p>　　分析上面的模型，出现负值，是因为少了对w的非负约束，那就加上：<br>$$<br>\min_{w}f(w)=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})\}<br>$$<br>$$<br>s.t.\quad w_m\ge 0,\quad m=1,2,…,M-1\\<br>1-\sum_{m=1}^{M-1}w_m\ge 0\qquad\qquad<br>$$<br>　　这就成了带约束的优化问题，该怎么解呢？是时候请出拉格朗日大神了！大神蜜汁微笑，抛出一套对偶宝典，将带约束的极小化问题改造成极大极小问题，正是</p>
<h2 id="u7B2C_u4E94_u5F0F_uFF1A_u6CA7_u6D77_u4E00_u7C9F_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E94_u5F0F_uFF1A_u6CA7_u6D77_u4E00_u7C9F_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第五式：沧海一粟！！！！！"></a>第五式：沧海一粟！！！！！</h2><p>　　我们先把问题标准化，假定$f(x)$，$c_i(x)$，$h_j(x)$ 是定义在$R^n$上的连续可微函数，将下面约束最优化问题<br>$$<br>\min_{x}f(x)\\<br>s.t.\quad c_i(x)\le 0,\quad i=1,2,…,k\\<br>\quad \quad h_j(x)=0,\quad j=1,2,…,l<br>$$<br>称为原始问题。</p>
<p>　　然后引入广义拉格朗日函数<br>$$<br>L(x,\alpha,\beta)=f(x)+\sum_{i=1}^{k}\alpha_{i}c_i(x)+\sum_{j=1}^{l}\beta_{j}h_j(x)<br>$$<br>　　其中 $\alpha_i$ 和 $\beta_j$ 是拉格朗日乘子，$\alpha_i\ge 0$。然后经过一番推来导去眉来眼去，可以证明<br>$$<br>\min_x\max_{\alpha,\beta:\alpha_i\ge 0}L(x,\alpha,\beta)<br>$$<br>与原始问题具有相同的x最优解，称作广义拉格朗日函数的极小极大问题。</p>
<p>　　又有，当$f(x)$，$c_i(x)$，$h_j(x)$ 等满足一定条件时<br>$$<br>\max_{\alpha,\beta:\alpha_i\ge 0}\min_xL(x,\alpha,\beta)<br>$$<br>也与原始问题具有相同的x最优解，称作广义拉格朗日函数的极大极小问题，可以将此问题也改写成约束的形式：<br>$$<br>\max_{\alpha,\beta}\theta_D(\alpha,\beta)=\max_{\alpha,\beta}\min_xL(x,\alpha,\beta)\\<br>s.t.\qquad \alpha_i\ge 0,\quad i=1,2,…,k<br>$$<br>称作原始问题的对偶问题。</p>
<p>　　这一部分的具体细节懒得写了，估计你们也看不下去，真要有兴趣可以去看书，比如《凸优化》或者李航老师的《统计学习方法》，我上面的公式基本就是抄他的，但请注意书上附录C的KKT条件有误，(C.22)式和(C.23)式不应该包含在里面。</p>
<p>　　好啦，照此框架，来解决我们的问题：<br>$$<br>\max_{\alpha,\alpha_m\ge 0}\min_wL(w,\alpha)\\<br>=\max_{\alpha,\alpha_m\ge 0}\min_w\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})-\sum_{m=1}^{M-1}\alpha_mw_m-\alpha_M(1-\sum_{m=1}^{M-1}w_m)\}<br>$$<br>　　极大极小公式列出来很容易，如何求解才头疼。</p>
<p>　　最早接触拉格朗日对偶是看SVM的推导，最近研究在线分配的shale算法又遇到它，发现基本都是依靠KKT条件推导，然而后面各有各的玩法，没有什么普适的方案。我也试着从KKT出发，但始终没找到什么高效的方法，一赌气，干脆舍弃KKT直接求解极大极小问题。如果哪位高人有更高明的招式，请一定传授小弟。</p>
<p>　　我自己想的招式便是以不变应万变，依然按sgd的思路，每来一条样本，先固定 $\alpha$ 不动，更新w，这是关于w的极小化问题，使用梯度下降（依然梯度杀）；然后固定w，更新 $\alpha$，这是关于 $\alpha$ 的极大化问题，使用梯度上升（可称梯云纵）。对 $\alpha$ 的非负约束也很简单，更新后如果小于0，则置为0。</p>
<p>　　如此交错曲折，在解空间的山坡上忽上忽下苦苦寻觅，“路漫漫其修远兮”，正是</p>
<h2 id="u7B2C_u516D_u5F0F_uFF1A_u4E0A_u4E0B_u6C42_u7D22_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u516D_u5F0F_uFF1A_u4E0A_u4E0B_u6C42_u7D22_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第六式：上下求索！！！！！！"></a>第六式：上下求索！！！！！！</h2><p>　　具体的梯度计算如下：<br>$$<br>\frac{\partial l_n(w|\alpha)}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}-\hat{y}_{nMk}}{\hat{y}_{nk}}-\alpha_m+\alpha_M,\quad m=1,…,M-1\\<br>\frac{\partial l_n(\alpha|w)}{\partial \alpha_m}=-w_m,\quad m=1,…,M-1\qquad\qquad\qquad\qquad\qquad\qquad\\<br>\frac{\partial l_n(\alpha|w)}{\partial \alpha_M}=-(1-\sum_{m=1}^{M-1}w_m)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad<br>$$<br>　　adagrad代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">n = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    g = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum-<span class="number">1</span>):</span><br><span class="line">        g[m] = (x[modelNum-<span class="number">1</span>][label]-x[m][label])/pre[label] - A[m] + A[modelNum-<span class="number">1</span>]</span><br><span class="line">        n[m] += g[m] * g[m]</span><br><span class="line">        lr = alfa/(beta+math.sqrt(n[m]))</span><br><span class="line">        w[m] -= lr * g[m]</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    w[modelNum-<span class="number">1</span>] = <span class="number">1</span>-wsum</span><br><span class="line">    gA = -w[modelNum-<span class="number">1</span>]</span><br><span class="line">    nA[modelNum-<span class="number">1</span>] += gA * gA</span><br><span class="line">    lrA = alfa/(beta+math.sqrt(nA[modelNum-<span class="number">1</span>]))</span><br><span class="line">    A[modelNum-<span class="number">1</span>] += lrA * gA</span><br><span class="line">    <span class="keyword">if</span> A[modelNum-<span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">        A[modelNum-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　看一下效果，还是以上面的gender二分类为例，结果如下：</p>
<pre><code>0.123262252313    gender_cut_ftrl8000_val
0.0782768467103   gender_cut_ftrl_val
0.0340907260294   gender_nn_1005_val
0.243746308063    gender_cut_ftrl2000_val
0.447138729994    gender_cut_ftrl1000_val
0.0627800144873   gender_newFM_val
0.00261889445352  gender_xxFM_val
0.00808622794875  gender_result_fm
</code></pre><p>　　可以看到gender_xxFM_val和gender_result_fm的权重终于不再是负数，而且很小，接近于0，跟期望的一样，最后的logloss也基本一致，等于0.435125。</p>
<p>　　说明我们的方法很给力啊！不能骄傲，继续前行。上面的方法对最后一项权重$w_M$ 做了特殊处理，不够优雅，如果我们不专门处理它，而是把权重之和归一放在约束里呢？</p>
<h2 id="u7B2C_u4E03_u5F0F_uFF1A_u4E07_u6CD5_u5F52_u4E00_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E03_u5F0F_uFF1A_u4E07_u6CD5_u5F52_u4E00_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第七式：万法归一！！！！！！！"></a>第七式：万法归一！！！！！！！</h2><p>　　重新设计模型<br>$$<br>\hat{y}_{nk}=\sum_{m=1}^{M}w_m\hat{y}_{nmk}<br>$$<br>$$<br>\min_{w}f(w)=<br>\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M}w_m\hat{y}_{nmk})\}\\<br>s.t.\qquad w_m\ge 0,\quad m=1,2,…,M\\<br>\sum_{m=1}^{M}w_m=1\qquad<br>$$<br>$$<br>L(w,\alpha,\beta)=-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M}w_m\hat{y}_{nmk})-\sum_{m=1}^{M}\alpha_mw_m+\beta(\sum_{m=1}^{M}w_m-1)<br>$$<br>　　梯度计算<br>$$<br>\frac{\partial l_n(w|\alpha,\beta)}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}}{\hat{y}_{nk}}-\alpha_m+\beta,\quad m=1,…,M\\<br>\frac{\partial l_n(\alpha|w,\beta)}{\partial \alpha_m}=-w_m,\quad m=1,…,M\qquad\qquad\qquad\qquad\qquad\\<br>\frac{\partial l_n(\beta|w,\alpha)}{\partial \beta}=\sum_{m=1}^{M}w_m-1\qquad\qquad\qquad\qquad\qquad\qquad\qquad<br>$$<br>　　代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">nw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">B = <span class="number">0.0</span></span><br><span class="line">nB = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        gw = -x[m][label]/pre[label] - A[m] + B</span><br><span class="line">        nw[m] += gw * gw</span><br><span class="line">        lrw = alfa/(beta+math.sqrt(nw[m]))</span><br><span class="line">        w[m] -= lrw * gw</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    gB = wsum - <span class="number">1</span></span><br><span class="line">    nB += gB * gB</span><br><span class="line">    lrB = alfa/(beta+math.sqrt(nB))</span><br><span class="line">    B += lrB * gB</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　效果</p>
<pre><code>0.117368795386    gender_cut_ftrl8000_val
0.0736857356383   gender_cut_ftrl_val
0.0387213072901   gender_nn_1005_val
0.236047170656    gender_cut_ftrl2000_val
0.463029142921    gender_cut_ftrl1000_val
0.069749655191    gender_newFM_val
0.00104464602712  gender_xxFM_val
0.00131312041348  gender_result_fm
</code></pre><p>　　注意，这里权重之和并不是严格的1，而是1.000959573523，毕竟是个迭代算法，没法保证严格的约束啊……所以计算logloss还是要重新归一下，最后logloss = 0.43512，跟前面的结果基本一致。</p>
<p>　　虽然最后两项的权重已经很接近于0，但看着还是很不爽，何不把很小的权重截成严格的0，就像生死符那样爽。对老司机来说都不是问题，上FTRL（为什么又是我）就好了嘛。</p>
<h2 id="u7B2C_u516B_u5F0F_uFF1A_u622A_u6743_u9053_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u516B_u5F0F_uFF1A_u622A_u6743_u9053_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第八式：截权道！！！！！！！！"></a>第八式：截权道！！！！！！！！</h2><p>　　FTRL的稀疏性就是专职干这的，将w的adagrad改成FTRL，代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">l1 = float(sys.argv[<span class="number">4</span>])</span><br><span class="line">l2 = float(sys.argv[<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"l1="</span> + str(l1)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"l2="</span> + str(l2)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">nw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">zw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">B = <span class="number">0.0</span></span><br><span class="line">nB = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    avgp = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">            avgp[k] += x[m][k]/modelNum</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        p = pre[label]</span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == p:</span><br><span class="line">            p = avgp[label]</span><br><span class="line">        gw = -x[m][label]/p - A[m] + B</span><br><span class="line">        sw = (math.sqrt(nw[m]+gw*gw)-math.sqrt(nw[m]))/alfa</span><br><span class="line">        zw[m] += gw - sw * w[m]</span><br><span class="line">        nw[m] += gw * gw</span><br><span class="line">        <span class="keyword">if</span> abs(zw[m]) &lt; l1:</span><br><span class="line">            w[m] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sgnz = <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> zw[m] &lt; <span class="number">0</span>:</span><br><span class="line">                sgnz = -<span class="number">1</span></span><br><span class="line">            w[m] = (sgnz*l1 - zw[m])/((beta + math.sqrt(nw[m]))/alfa + l2)</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    gB = wsum - <span class="number">1</span></span><br><span class="line">    nB += gB * gB</span><br><span class="line">    lrB = alfa/(beta+math.sqrt(nB))</span><br><span class="line">    B += lrB * gB</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　注意这里L1正则要设的很大才行，比如100，效果如下：</p>
<pre><code>0.117563109491  gender_cut_ftrl8000_val
0.0734000016105 gender_cut_ftrl_val
0.0378296537773 gender_nn_1005_val
0.234723810866  gender_cut_ftrl2000_val
0.461201320728  gender_cut_ftrl1000_val
0.0747847726281 gender_newFM_val
0               gender_xxFM_val
0               gender_result_fm
</code></pre><p>　　哈哈，最后两项终于被彻底干掉，最后的logloss = 0.435117，依然保持的很好。</p>
<p>　　以上所有方法，在gender的7分类上也做了实验，同样有效，懒得再贴结果。<br><br><br>　　故事终于讲完了，Ensembling met Lagrange, and they lived happily ever after.<br><br><br><br><br>上一篇转到微信公众号时加了段作者简介：<br><small>本文作者：张卫，8年多码农生涯，先后在腾讯、搜狗混过些时日，目前在小米负责广告算法。无甚特别，唯好数学物理，正所谓推公式无敌手，推妹子无得手的中二汉子。</small></p>
<p>结果被人吐槽成了征婚帖，好吧，这次改一改：</p>
<p><small>本文作者：张卫，8年多码农生涯，先后在腾讯、搜狗混过些时日，目前在小米负责广告算法。曾经年少轻狂，颇好武侠旧学，晨舞葬诗魂的冷月刀，暮弄渡鹤影的寒塘剑；而今终日代码公式为伍，间或寄情文字，重举旧时觞。</small></p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/11/22/ensembling_lagrange/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/10/16/fm_ftrl_softmax/">FM, FTRL, Softmax</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-10-16T05:52:45.000Z">Oct 16 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="FM_2C_FTRL_2C_Softmax"><a href="#FM_2C_FTRL_2C_Softmax" class="headerlink" title="FM, FTRL, Softmax"></a>FM, FTRL, Softmax</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　最近公司内部举办了一届数据挖掘大赛，题目是根据用户的一些属性和行为数据来预测性别和年龄区间，属于一个二分类问题（性别预测男女）和一个多分类问题（年龄分为7个区间），评判标准为logloss。共有五六十支队伍提交，我们组的三名小伙伴最终取得第三名的好成绩，跟前两名只有千分之一二的差距。</p>
<p>　　赛后总结，发现前6名全部使用了DNN模型，而我们团队比较特别的是，不只使用了DNN，还有FM，最终方案是六七个DNN模型和一个FM模型的ensembling。</p>
<p>　　其实比赛刚开始，他们使用的是XGBoost，因为XGBoost的名头实在太响。但这次比赛的数据量规模较大，训练样本数达到千万，XGBoost跑起来异常的慢，一个模型要跑一两天。于是我把几个月前写的FM工具给他们用，效果非常好，二分类只需十几分钟，多分类也就半个多小时，logloss和XGBoost基本持平，甚至更低。最终他们抛弃了XGBoost，使用FM在快速验证特征和模型融合方面都起到了很好的作用。此外，我们组另外两名实习生仅使用此FM工具就取得了第七名的成绩。</p>
<p>　　最初写此FM代码时正值alphaGo完虐人类，因此随手给这个工具起了个名字叫alphaFM，今天我就来分享一下这个工具是如何实现的。</p>
<h2 id="alphaFM_u4ECB_u7ECD"><a href="#alphaFM_u4ECB_u7ECD" class="headerlink" title="alphaFM介绍"></a>alphaFM介绍</h2><p>　　代码地址在：<br>　　<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">https://github.com/CastellanZhang/alphaFM</a><br>　　<a href="https://github.com/CastellanZhang/alphaFM_softmax" target="_blank" rel="external">https://github.com/CastellanZhang/alphaFM_softmax</a></p>
<p>　　alphaFM是Factorization Machines的一个单机多线程版本实现，用于解决二分类问题，比如CTR预估，优化算法采用了FTRL。我其实把sgd和adagrad的方法也实现了，但最终发现还是FTRL的效果最好。</p>
<p>　　实现alphaFM的初衷是解决大规模数据的FM训练，在我们真实的业务数据中，训练样本数常常是千万到亿级别，特征维度是百万到千万级别甚至上亿，这样规模的数据完全加载到内存训练已经不太现实，甚至下载到本地硬盘都很困难，一般都是经过spark生成样本直接存储在hdfs上。</p>
<p>　　alphaFM用于解决这样的问题特别适合，一边从hdfs下载，一边计算，一个典型的使用方法是这样：</p>
<p>　　训练：10个线程计算，factorization的维度是8，最后得到模型文件fm_model.txt</p>
<p>　　<code>hadoop fs -cat train_data_hdfs_path | ./fm_train -core 10 -dim 1,1,8 -m fm_model.txt</code></p>
<p>　　测试：10个线程计算，factorization的维度是8，加载模型文件fm_model.txt，最后输出预测结果文件fm_pre.txt</p>
<p>　　<code>hadoop fs -cat test_data_hdfs_path | ./fm_predict -core 10 -dim 8 -m fm_model.txt -out fm_pre.txt</code></p>
<p>　　当然，如果样本文件不大，也可以先下载到本地，然后再运行alphaFM。</p>
<p>　　由于采用了FTRL，调好参数后，训练样本只需过一遍即可收敛，无需多次迭代，因此alphaFM读取训练样本采用了管道的方式，这样的好处除了节省内存，还可以通过管道对输入数据做各种中间过程的转换，比如采样、格式变换等，无需重新生成训练样本，方便灵活做实验。</p>
<p>　　alphaFM还支持加载上次的模型，继续在新数据上训练，理论上可以一直这样增量式进行下去。</p>
<p>　　FTRL的好处之一是可以得到稀疏解，在LR上非常有效，但对于FM，模型参数v是个向量，对于每一个特征，必须w为0且v的每一维都为0才算稀疏解， 但这通常很难满足，所以加了一个force_v_sparse的参数，在训练过程中，每当w变成0时，就强制将对应的v变成0向量。这样就可以得到很好的稀疏效果，且在我的实验中发现最终对test样本的logloss没有什么影响。</p>
<p>　　当将dim参数设置为1,1,0时，alphaFM就退化成标准的LR的FTRL训练工具。不禁想起我们最早的LR的FTRL代码还是勇保同学写的，我现在的代码基本上还是沿用了当初的多线程思路，感慨一下。</p>
<p>　　alphaFM能够处理的特征维度取决于内存大小，训练样本基本不占内存，理论上可以处理任意多的数量。后续可以考虑基于ps框架把alphaFM改造成分布式版本，这样就可以支持更大的特征维度。</p>
<p>　　alphaFM_softmax是alphaFM的多分类版本。两个工具的具体使用方法和参数说明见代码的readme，这里不再详述。</p>
<p>　　接下来请各位打起精神，我们来推一推公式。诗云，万丈高楼平地起，牛不牛逼靠地基。公式就是算法工具的地基，公式整明白了，像我们这种”精通”C++的（谁简历里不是呢:-P），实现就是分分钟的事（装B中，勿扰：-）。</p>
<h2 id="u4E8C_u5206_u7C7B_u95EE_u9898"><a href="#u4E8C_u5206_u7C7B_u95EE_u9898" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>　　对于二分类，最常见的模型是LR，搭配FTRL优化算法。LR的输出会用到sigmoid函数，定义为：<br>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}<br>$$<br>　　LR预测输入$x$是正样本的概率：<br>$$<br>P(y=1|x,w)=\frac{1}{1+e^{-w^Tx}}=\sigma(w^Tx)<br>$$<br>　　可以看到，$\sigma$函数的参数部分 $w^Tx$ 是一个线性函数，这也就是LR被称作线性模型的原因，模型参数只有一个$w$向量，相对简单。如果我们把这部分弄复杂呢？比如这样：<br>$$<br>\hat{y}(x|\Theta):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\langle v_i,v_j\rangle x_ix_j\\<br>=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nx_ix_j\sum_{f=1}^kv_{i,f}v_{j,f}\\<br>=w_0+\sum_{i=1}^nw_ix_i+\frac{1}{2}\sum_{f=1}^k\left(\left(\sum_{i=1}^nv_{i,f}x_i\right)^2-\sum_{i=1}^nv_{i,f}^2x_i^2\right)<br>$$<br>　　其中，$x\in R^n$，$w_0\in R$，$w\in R^n$，$V\in R^{n\times k}$，这其实就是一个2阶FM，模型参数 $\Theta=\{w_0,w_1,…,w_n,v_{1,1},…,v_{n,k}\}$ 。如果直接将 $\hat{y}(x|\Theta)$ 做输出，采用平方损失函数便可解决回归问题。而对于二分类问题，外面套一个sigmoid函数即可：<br>$$<br>P(y=1|x,\Theta)=\frac{1}{1+e^{-\hat{y}(x|\Theta)}}<br>$$<br>　　对于$y\in \{-1,1\}$，可统一成形式：<br>$$<br>P(y|x,\Theta)=\frac{1}{1+e^{-y\hat{y}(x|\Theta)}}=\sigma(y\hat{y}(x|\Theta))<br>$$<br>　　模型参数估计采用最大似然的方法，对于训练数据$S$，最优化问题为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(x,y)\in S}P(y|x,\Theta)=\mathop{\arg\min}_{\Theta}\sum_{(x,y)\in S}-\ln P(y|x,\Theta)<br>$$<br>　　即样本 $(x,y)$ 的损失函数为：<br>$$<br>l(\Theta|x,y)=-\ln P(y|x,\Theta)=-\ln \sigma(y\hat{y}(x|\Theta))<br>$$<br>　　此损失函数对 $\hat{y}$ 求偏导会得到一个优雅简单的形式：<br>$$<br>\frac{\partial l}{\partial\hat{y}}=y(\sigma(y\hat{y})-1)<br>$$<br>　　再配合上 $\hat{y}$ 对模型参数的偏导：<br>$$<br>\frac{\partial\hat{y}}{\partial\theta}=<br>\begin{cases}<br>  1,   &amp; if\,\,\theta\,\,is\,\,w_0 \\<br>  x_i, &amp; if\,\,\theta\,\,is\,\,w_i \\<br>  x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 &amp; if\,\,\theta\,\,is\,\,v_{i,f} \\<br>\end{cases}<br>$$<br>　　便可得到损失函数 $l$ 对所有模型参数的偏导，即：<br>$$<br>g_0^w=\frac{\partial l}{\partial w_0}=y(\sigma(y\hat{y})-1)\\<br>g_i^w=\frac{\partial l}{\partial w_i}=y(\sigma(y\hat{y})-1)x_i\\<br>g_{i,f}^v=\frac{\partial l}{\partial v_{i,f}}=y(\sigma(y\hat{y})-1)(x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2)<br>$$</p>
<p>此时，我们能够很自然的想到用SGD的方法来求解模型参数，但我这里采用了更加高效的FTRL优化算法。</p>
<p>　　让我们来简单回顾一下FTRL，Google在2013年放出这个优化方法，迅速火遍大江南北，原始论文里只是用来解决LR问题，论文截图如下：</p>
<p><img src="/img/ftrl.jpg" alt=""></p>
<p>　　但其实FTRL是一个online learning的框架，能解决的问题绝不仅仅是LR，已经成了一个通用的优化算子，比如TensorFlow的optimizer中都包含了FTRL。我们只要把截图中的伪代码修改，$p_t$的计算改为 $\hat{y}(x|\Theta)$，对于每一轮的特征向量$x$的每一维非0特征$x_i$，都要相应的更新模型参数$w_0,w_i,v_{i,1},…,v_{i,k}$，更新公式不变和截图一致，梯度$g$的计算即为损失函数对每个参数的偏导，前面已经给出。$\sigma,z,n$的更新公式不变。伪代码如下：</p>
<hr>
<p>Algorithm: alphaFM</p>
<hr>
<p>$Input:paramters\,\alpha^w,\alpha^v,\beta^w,\beta^v,\lambda_1^w,\lambda_1^v,\lambda_2^w,\lambda_2^v,\sigma$<br>$Init:w_0=0;n_0^w=0;z_0^w=0;$<br>$Init:\forall i,\forall f,w_i=0;n_i^w=0;z_i^w=0;v_{i,f}\sim N(0,\sigma);n_{i,f}^v=0;z_{i,f}^v=0;$<br>$for\,t=1\,to\,T,do$<br>$\qquad Receive\,feature\,vector\,x\,and\,let\,I=\{i|x_i\neq0\}$<br>$$<br>w_0=<br>\begin{cases}<br>  0   &amp; if\,\,|z_0^w|\le\lambda_1^w \\<br>  -\left(\frac{\beta^w+\sqrt{n_0^w}}{\alpha^w}+\lambda_2^w\right)^{-1}(z_0^w-sgn(z_0^w)\lambda_1^w) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad for\,i\in I,compute$<br>$$<br>w_i=<br>\begin{cases}<br>  0   &amp; if\,\,|z_i^w|\le\lambda_1^w \\<br>  -\left(\frac{\beta^w+\sqrt{n_i^w}}{\alpha^w}+\lambda_2^w\right)^{-1}(z_i^w-sgn(z_i^w)\lambda_1^w) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad \qquad for\,f=1\,to\,k,compute$<br>$$<br>v_{i,f}=<br>\begin{cases}<br>  0   &amp; if\,\,|z_{i,f}^v|\le\lambda_1^v \\<br>  -\left(\frac{\beta^v+\sqrt{n_{i,f}^v}}{\alpha^v}+\lambda_2^v\right)^{-1}(z_{i,f}^v-sgn(z_{i,f}^v)\lambda_1^v) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad \qquad end\,for$<br>$\qquad end\,for$<br>$\qquad Compute\,\hat{y}(x|\Theta)$<br>$\qquad Observe\,label\,y\in\{-1,1\}$<br>$\qquad compute\,g_0^w$<br>$\qquad \sigma_0^w=\frac{1}{\alpha^w}(\sqrt{n_0^w+(g_0^w)^2}-\sqrt{n_0^w})$<br>$\qquad z_0^w\leftarrow z_0^w+g_0^w-\sigma_0^ww_0$<br>$\qquad n_0^w\leftarrow n_0^w+(g_0^w)^2$<br>$\qquad for\,i\in I,do$<br>$\qquad \qquad compute\,g_i^w$<br>$\qquad \qquad \sigma_i^w=\frac{1}{\alpha^w}(\sqrt{n_i^w+(g_i^w)^2}-\sqrt{n_i^w})$<br>$\qquad \qquad z_i^w\leftarrow z_i^w+g_i^w-\sigma_i^ww_i$<br>$\qquad \qquad n_i^w\leftarrow n_i^w+(g_i^w)^2$<br>$\qquad \qquad for\,f=1\,to\,k,do$<br>$\qquad \qquad \qquad compute\,g_{i,f}^v$<br>$\qquad \qquad \qquad \sigma_{i,f}^v=\frac{1}{\alpha^v}(\sqrt{n_{i,f}^v+(g_{i,f}^v)^2}-\sqrt{n_{i,f}^v})$<br>$\qquad \qquad \qquad z_{i,f}^v\leftarrow z_{i,f}^v+g_{i,f}^v-\sigma_{i,f}^vv_{i,f}$<br>$\qquad \qquad \qquad n_{i,f}^v\leftarrow n_{i,f}^v+(g_{i,f}^v)^2$<br>$\qquad \qquad end\,for$<br>$\qquad end\,for$<br>$end\,for$    </p>
<hr>
<h2 id="u591A_u5206_u7C7B_u95EE_u9898"><a href="#u591A_u5206_u7C7B_u95EE_u9898" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>　　Softmax模型是LR在多分类上的推广，具体介绍戳<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="external">这里</a>。大致就是如果有$c$个类别，则模型参数为$c$个向量：$\Theta=\{w_1,w_2,…,w_c\}$，其中任意$w_i\in R^n$。</p>
<p>　　样本$x$属于类别$i$的概率：<br>$$<br>P(y=i|x,\Theta)=\frac{e^{w_i^Tx}}{\sum_{j=1}^ce^{w_j^Tx}}<br>$$<br>　　FM解决多分类的方法同样是将线性部分$w^Tx$替换成复杂的 $\hat{y}(x|\Theta)$，不过不再是一个 $\hat{y}$，而是每一类别对应一个，共$c$个：$\hat{y}_1(x|\Theta),…,\hat{y}_c(x|\Theta)$</p>
<p>　　样本$x$属于类别$i$的概率也变成：<br>$$<br>P(y=i|x,\Theta)=\frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}<br>$$<br>　　模型参数一共$c$组， $\Theta=\{\Theta_1,…,\Theta_c\}$，其中类别$i$对应一组参数 $\Theta_i=\{w_0^i,w_1^i,…,w_n^i,v_{1,1}^i,…,v_{n,k}^i\}$</p>
<p>　　我们定义一个示性函数 $1\{\cdot\}$，大括号中表达式为真则值为1，表达式为假则值为0。这样就可以写出最优化问题：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(x,y)\in S}\prod_{i=1}^cP(y=i|x,\Theta)^{1\{y=i\}}\\<br>=\mathop{\arg\min}_{\Theta}-\sum_{(x,y)\in S}\sum_{i=1}^c1\{y=i\}\ln P(y=i|x,\Theta)<br>$$<br>　　每条样本 $(x,y)$ 的损失函数：<br>$$<br>l(\Theta|x,y)=-\sum_{i=1}^c1\{y=i\}\ln P(y=i|x,\Theta)\\<br>=-\sum_{i=1}^c1\{y=i\}\ln \frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}\\<br>=\sum_{i=1}^c1\{y=i\}(\ln\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}-\hat{y}_i(x|\Theta))\\<br>=\ln\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}-\sum_{i=1}^c1\{y=i\}\hat{y}_i(x|\Theta)<br>$$<br>　　梯度：<br>$$<br>\nabla_{\Theta_i}l(\Theta|x,y)=\frac{\partial l}{\partial\hat{y}_i}\nabla_{\Theta_i}\hat{y}_i(x|\Theta)<br>$$<br>　　而<br>$$<br>\frac{\partial l}{\partial\hat{y}_i}=\frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}-1\{y=i\}\\<br>=P(y=i|x,\Theta)-1\{y=i\}<br>$$<br>　　所以有<br>$$<br>\nabla_{\Theta_i}l(\Theta|x,y)=(P(y=i|x,\Theta)-1\{y=i\})\nabla_{\Theta_i}\hat{y}_i(x|\Theta)<br>$$<br>$\nabla_{\Theta_i}\hat{y}_i(x|\Theta)$ 即求 $\hat{y}_i$ 对 $\Theta_i$ 中所有参数 $\{w_0^i,w_1^i,…,w_n^i,v_{1,1}^i,…,v_{n,k}^i\}$ 的偏导，这在二分类中我们已经给出。</p>
<p>　　最后，仍然是套用FTRL的框架，只是每条样本更新的参数更多，不再细说，详见代码。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/10/16/fm_ftrl_softmax/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/02/05/cf_als/">CF的ALS算法推导</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-02-05T03:47:45.000Z">Feb 5 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>　　在上一篇中介绍了矩阵微分，现在就来牛刀小试一下。早些时候子龙问过我 Collaborative Filtering for Implicit Feedback Datasets这篇论文里的公式推导，在这里重新解一遍。<br>　　论文里给出的目标函数为：<br>$$<br>\min_{x_*,y_*}\left\{\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda(\sum_u||x_u||^2+\sum_i||y_i||^2)\right\}\,\,\,\,(1)<br>$$<br>　　模型的来历不是今天的重点，简单描述一下各个变量的含义，其中$u$代表user，$i$代表item，$x_u\in R^f$，$y_i\in R^f$，未出现的变量 $r_{ui}$ 表示user对item的“消费”度量，<br>$$<br>p_{ui}=<br>\begin{cases}<br>  1 &amp; r_{ui}&gt;0 \\<br>  0 &amp; r_{ui}=0 \\<br>\end{cases}<br>$$<br>表征user是否对item有偏好，$c_{ui}=1+\alpha r_{ui}$ 表征$p_{ui}$ 的置信度。<br>　　假设user的数目为$m$，item的数目为$n$，则目标函数包含的项大约有$m\cdot n$，通常数量非常巨大，一些常见的优化算法比如SGD不再适合，论文中采用了ALS（alternating-least-squares）算法，这是一种迭代的方法，第一步先固定所有的$y_i$，求解最优的$x_u$；第二步固定所有的$x_u$，求解最优的$y_i$，依次迭代。<br>　　我们先看第一步，当固定所有$y_i$后，(1)式转化为<br>$$<br>\min_{x_*}\left\{\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda\sum_u||x_u||^2\right\}\\<br>=\sum_u\min_{x_u}\left\{\sum_{i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda||x_u||^2\right\}\\<br>=\sum_u\min_{x_u}L_u(x_u)<br>$$<br>问题简化为求每个$L_u(x_u)$ 函数的最小值。<br>　　对每个$u$，我们定义一个$n\times n$的对角矩阵$C^u$，其中$C_{ii}^u=c_{ui}$，定义向量$p(u)\in R^n$包含所有的$p_{ui}$。激动人心的时刻来到了，我们开始推导函数$L_u(x_u)$ 对$x_u$的微分<br>$$<br>L_u(x_u)=\sum_{i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda||x_u||^2\\<br>=<br>\sum_{i}c_{ui}(y_i^Tx_u-p_{ui})^2+\lambda x_u^Tx_u\\<br>dL_u=\sum_{i}2c_{ui}(x_u^Ty_i-p_{ui})y_i^Tdx_u+2\lambda x_u^Tdx_u\\<br>=2\left(\sum_{i}c_{ui}x_u^Ty_iy_i^T-\sum_{i}c_{ui}p_{ui}y_i^T+\lambda x_u^T\right)dx_u\\<br>$$<br>　　$L_u(x_u)$ 取极值，须$dL_u=0$，有<br>$$<br>\sum_{i}c_{ui}x_u^Ty_iy_i^T-\sum_{i}c_{ui}p_{ui}y_i^T+\lambda x_u^T=0\\<br>\Rightarrow \sum_{i}c_{ui}y_iy_i^Tx_u+\lambda Ix_u=\sum_{i}c_{ui}p_{ui}y_i\\<br>\Rightarrow \left(\sum_{i}c_{ui}y_iy_i^T+\lambda I\right)x_u=\sum_{i}c_{ui}p_{ui}y_i\,\,\,\,(2)<br>$$<br>　　按论文中的符号定义，设$Y^T=(y_1,\cdots,y_n)$，有<br>$$<br>\sum_{i}c_{ui}y_iy_i^T=(c_{u1}y_1,\cdots,c_{un}y_n)<br>\left(\begin{array}{c}<br>  y_1^T \\<br>  \vdots \\<br>  y_n^T \\<br>\end{array}\right)\\<br>=(y_1,\cdots,y_n)<br>\left(\begin{array}{ccc}<br>  c_{u1} &amp;  &amp;  \\<br>  &amp; \ddots &amp;  \\<br>  &amp;  &amp;  c_{un} \\<br>\end{array}\right)<br>\left(\begin{array}{c}<br>  y_1^T \\<br>  \vdots \\<br>  y_n^T \\<br>\end{array}\right)\\<br>=Y^TC^uY<br>$$<br>　　类似地，有 $\sum_{i}c_{ui}p_{ui}y_i=Y^TC^up(u)$，因此<br>$$<br>(2)\Rightarrow (Y^TC^uY+\lambda I)x_u=Y^TC^up(u)\\<br>\Rightarrow x_u=(Y^TC^uY+\lambda I)^{-1}Y^TC^up(u)<br>$$<br>正是论文中的结果。对于固定$x_u$求$y_i$完全类似，不再详述。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/02/05/cf_als/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>




  <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/02/02/matrix_differential/">Matrix Differential</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-02-02T12:45:11.000Z">Feb 2 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="u77E9_u9635_u5FAE_u5206_28Matrix_Differential_29"><a href="#u77E9_u9635_u5FAE_u5206_28Matrix_Differential_29" class="headerlink" title="矩阵微分(Matrix Differential)"></a>矩阵微分(Matrix Differential)</h1><p>　　在最优化问题中，经常涉及到导数或梯度的计算，比如 $\nabla(x’Ax)$，虽然自己最终也能推导出来等于 $(A+A’)x$，但总感觉在这块缺少系统的知识，有时还会想向量函数的导数又如何计算，矩阵函数呢？在网上搜了很久，似乎这块的知识点一直比较混乱，wikipedia的Matrix calculus词条也写得不好，甚至有人质疑。大约一年前又去中关村图书大厦遍寻矩阵相关的书籍，只找到清华出的一本《矩阵分析与应用》涉及到矩阵微分，手机拍了几十张照片回来细读仍不甚满意。无意间却从网上找到了这本书：Matrix Differential Calculus with Applications in Statistics and Econometrics，正是我想要的！（其实清华那本和wikipedia都提到了这本书，只是当时没注意。）书甚厚，博大精深，我把感兴趣的部分边看边做笔记，唯纸薄字陋难存久，便想着不如写成blog，可惜本是懒惰之人一拖便是一年，直到最近失意百无聊赖，终于完成。文章憎命达，诚如是。</p>
<h2 id="1-__u57FA_u7840_u6982_u5FF5"><a href="#1-__u57FA_u7840_u6982_u5FF5" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h2><p>　　向量$x$的范数(norm)定义为：<br>$$||x||=(x’x)^{1/2}$$</p>
<p>　　假设$a$为$n \times 1$向量，$A$为$n \times n$矩阵，$B$为$n \times m$矩阵，则$a’x$称为向量$x$的线性型，$x’Ax$称为$x$的二次型，$x’By$称为$x$和$y$的双线性型。<br>　　不失一般性，我们假定$A$为对称阵，因为我们总可以用 $(A+A’)/2$ 来代替，推导如下：<br>$$x’(A+A’)x=x’Ax+x’A’x=x’Ax+(x’A’x)’=x’Ax+x’Ax=2x’Ax$$<br>　　我们说，$A$是正定的，如果对所有$x\ne0$，有$x’Ax&gt;0$；$A$是半正定的，如果对所有$x$，有$x’Ax\ge0$。负定的定义类似，不再赘述。<br>　　易证 $BB’$ 和 $B’B$ 都是半正定的，因为 $x’B’Bx=(Bx)’Bx\ge0$  </p>
<p>　　方阵的迹(trace)定义为：</p>
<p>$$trA=\sum_{i=1}^na_{ii}$$</p>
<p>　　矩阵的范数定义为：</p>
<p>$$||A||=(trA’A)^{1/2}$$</p>
<p>　　矩阵的直积：<br>　　$A$为$m\times n$矩阵，$B$为$p\times q$矩阵，则$A\otimes B$为$mp\times nq$的矩阵<br>$$<br>A\otimes B=<br>\left(\begin{array}{ccc}<br>  a_{11}B &amp; \cdots &amp; a_{1n}B \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  a_{m1}B &amp; \cdots &amp; a_{mn}B \\<br>\end{array}\right)<br>$$</p>
<p>　　VEC算子：<br>　　设$A$为$m\times n$矩阵，$a_i$为其第$i$列，则$vec\,A$为$mn\times 1$向量<br>$$<br>vec\,A=<br>\left(\begin{array}{c}<br>  a_1 \\<br>  a_2 \\<br>  \vdots \\<br>  a_n<br>\end{array}\right)<br>$$<br>　　几个相关公式:<br>$$vec\,a’=vec\,a=a$$<br>$$vec\,ab’=b\otimes a$$<br>$$(vec\,A)’vec\,B=tr\,A’B$$<br>$$vec\,ABC=(C’\otimes A)vec\,B$$<br>$$vec\,AB=(B’\otimes I_m)vec\,A=(B’\otimes A)vec\,I_n=(I_q\otimes A)vec\,B,\,\,\,\,A:m\times n,B:n\times q$$</p>
<h2 id="2-__u5FAE_u5206_28differential_29_u7684_u5B9A_u4E49"><a href="#2-__u5FAE_u5206_28differential_29_u7684_u5B9A_u4E49" class="headerlink" title="2. 微分(differential)的定义"></a>2. 微分(differential)的定义</h2><h3 id="2-1__u6807_u91CF_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-1__u6807_u91CF_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.1 标量函数的微分"></a>2.1 标量函数的微分</h3><p>　　对于一维的情况，$\phi(x):R\rightarrow R$<br>$$\phi(c+u)=\phi(c)+u\phi’(c)+r_c(u)$$<br>$$\lim_{u\rightarrow 0}\frac{r_c(u)}{u}=0$$<br>　　定义 $\phi$ 在$c$点基于增量$u$的一阶微分为<br>$$d\phi(c;u)=u\phi’(c)$$</p>
<h3 id="2-2__u5411_u91CF_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-2__u5411_u91CF_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.2 向量函数的微分"></a>2.2 向量函数的微分</h3><p>　　设函数$f:S\rightarrow R^m$，$S\subset R^n$，$c$是$S$的一个内点(interior point)，$B(c;r)$ 是$S$中$c$点的一个邻域(n-ball)，$u$是$R^n$中的一点，满足 $||u||&lt;r$，因此有$c+u\in B(c;r)$，如果存在一个$m\times n$实矩阵$A$，满足<br>$$f(c+u)=f(c)+A(c)u+r_c(u)$$<br>对于所有的$u\in R^n$，$||u||&lt;r$，且<br>$$\lim_{u\rightarrow0}\frac{r_c(u)}{||u||}=0$$<br>这样，函数$f$就被称为在$c$点可微。矩阵$A(c)$ 称为$f$在$c$点的一阶导数。$m\times 1$向量<br>$$df(c;u)=A(c)u$$<br>称为$f$在$c$点的一阶微分（基于增量$u$）。</p>
<h3 id="2-3__u5411_u91CF_u51FD_u6570_u7684_u504F_u5BFC_u6570"><a href="#2-3__u5411_u91CF_u51FD_u6570_u7684_u504F_u5BFC_u6570" class="headerlink" title="2.3 向量函数的偏导数"></a>2.3 向量函数的偏导数</h3><p>　　令$f=(f_1,f_2,\cdots,f_m)$，$t\in R$，如果极限<br>$$\lim_{t\rightarrow0}\frac{f_i(c+te_j)-f_i(c)}{t}$$<br>存在，则称之为$f_i$在$c$点的第$j$个偏导数，记为$D_jf_i(c)$，或者 $[\partial f_i(x)/\partial x_j]_{x=c}$ 或者 $\partial f_i(c)/\partial x_j$。</p>
<p>　　如果$f$在$c$点可微，则所有的偏导数$D_jf_i(c)$ 存在。反过来不一定成立。</p>
<p>　　如果$f$在$c$点可微，那么存在矩阵$A(c)$，<br>$$f(c+u)=f(c)+A(c)u+r_c(u)$$<br>这里忽略了一些限制条件，详见前面向量的微分。<br>　　$A(c)$ 的每一项$a_{ij}(c)$ 其实就是相应的偏导数$D_jf_i(c)$，即<br>$$A(c)=Df(c)$$<br>这里的$Df(c)$ 被称作雅可比矩阵(Jacobian matrix)，注意$A(c)$ 存在时$Df(c)$ 一定存在，反之未必。  </p>
<h3 id="2-4__u68AF_u5EA6_28gradient_29_u7684_u5B9A_u4E49"><a href="#2-4__u68AF_u5EA6_28gradient_29_u7684_u5B9A_u4E49" class="headerlink" title="2.4 梯度(gradient)的定义"></a>2.4 梯度(gradient)的定义</h3><p>　　$Df(c)$ 的转置称为$f$在$c$点的梯度，用 $\nabla f(c)$ 表示，即<br>$$\nabla f(c)=(Df(c))’$$<br>　　当向量函数$f:S\rightarrow R^m$ 退化成标量函数 $\phi:S\rightarrow R$ 时，雅可比矩阵退化成$1\times n$行向量$D\phi(c)$，梯度退化成$n\times 1$列向量 $\nabla\phi(c)$。</p>
<h3 id="2-5__u77E9_u9635_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-5__u77E9_u9635_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.5 矩阵函数的微分"></a>2.5 矩阵函数的微分</h3><p>　　现在终于轮到矩阵函数出场了，令 $F:S\rightarrow R^{m\times p}$，其中 $S\subset R^{n\times q}$。$C$是$S$的内点，且令 $B(C;r)=\{X:X\in R^{n\times q},||X-C||&lt;r\}$，这里 $||X||=(trX’X)^{1/2}$。<br>　　设点$U$是$R^{n\times q}$ 内一点，满足 $||U||&lt;r$，因此有 $C+U\in B(C;r)$。如果存在一个 $mp\times nq$ 的矩阵 $A$，有<br>$$vec\,F(C+U)=vec\,F(C)+A(C)vec\,U+vec\,R_C(U)$$<br>对于所有 $U\in R^{n\times q}$，$||U||&lt;r$，且<br>$$\lim_{U\rightarrow0}\frac{R_C(U)}{||U||}=0$$<br>那么，函数 $F$ 被称为在 $C$ 点可微。<br>　　$m\times p$ 矩阵 $dF(C;U)$ 由下式定义<br>$$vec\,dF(C;U)=A(C)vec\,U$$<br>被称作 $F$ 在 $C$ 点基于增量 $U$ 的一阶微分，$mp\times nq$ 矩阵 $A(C)$ 被称作 $F$ 在 $C$ 点的一阶导数。</p>
<p>　　可以看到，这里的做法就是将矩阵函数化为向量函数来处理。对于每个矩阵函数 $F$（自变量和函数值均为矩阵），<br>我们都可以构造一个对应的向量函数$f:vec\,S\rightarrow R^{mp}$（自变量和函数值均为向量）<br>$$f(vec\,X)=vec\,F(X)$$<br>易得<br>$$vec\,dF(C;U)=df(vec\,C;vec\,U)$$<br>我们定义 $F$ 在 $C$ 点的雅可比矩阵为<br>$$DF(C)=Df(vec\,C)$$<br>这是一个 $mp\times nq$ 的矩阵，其第 $ij$ 元素是$vec\,F$的第 $i$ 个分量对 $vec\,X$ 的第 $j$个分量在 $X=C$ 处的偏导数。</p>
<p>　　当 $F$ 在 $C$ 点可微，有 $DF(C)=A(C)$</p>
<p>　　设 $U$ 和 $V$ 是矩阵函数，$A$ 是矩阵常量，有<br>$$dA=0$$<br>$$d(\alpha U)=\alpha dU$$<br>$$d(U+V)=dU+dV$$<br>$$d(U-V)=dU-dV$$<br>$$d(UV)=(dU)V+UdV$$<br>$$dU’=(dU)’$$<br>$$d\,vec\,U=vec\,dU$$<br>$$d\,tr\,U=tr\,dU$$</p>
<h3 id="2-6__u94FE_u5F0F_u6CD5_u5219"><a href="#2-6__u94FE_u5F0F_u6CD5_u5219" class="headerlink" title="2.6 链式法则"></a>2.6 链式法则</h3><p>　　同最简单的标量自变量的标量函数一样，链式法则同样成立。<br>　　设函数$f:S\rightarrow R^m$，$S\subset R^n$，在 $c$ 点可微。$f(x)\in T$，设函数$g:T\rightarrow R^p$在 $b=f(c)$ 点可微。定义复合函数$h:S\rightarrow R^p$ 如下：<br>$$<br>h(x)=g(f(x))<br>$$<br>那么，$h$ 在 $c$ 点可微，并且<br>$$<br>Dh(c)=(Dg(b))(Df(c))<br>$$<br>$$<br>dh(c;u)=(Dg(b))(Df(c))u=(Dg(b))df(c;u)=dg(b;df(c;u))<br>$$<br>　　对矩阵函数类似，省略掉函数定义，直接给出公式：<br>$$<br>H(X)=G(F(X))<br>$$<br>$$<br>DH(C)=(DG(B))(DF(C))<br>$$<br>$$<br>dH(C;U)=dG(B;dF(C;U))<br>$$<br>　　我们还可以简写微分符号：<br>$$<br>y=g(t)<br>$$<br>$$<br>dy=dg(t;dt)<br>$$<br>$$<br>t=f(x)<br>$$<br>$$<br>y=g(f(x))\equiv h(x)<br>$$<br>$$<br>dy=dh(x;dx)=dg(f(x);df(x;dx))=dg(t;dt)<br>$$<br>　　我们甚至可以不用$y$直接用$g$本身来简写：<br>$$<br>dg=dg(t;dt)<br>$$<br>　　举例：<br>$$<br>y=\phi(x)=e^{x’x}\\<br>dy=de^{x’x}=e^{x’x}(dx’x)=e^{x’x}((dx)’x+x’dx)=(2e^{x’x}x’)dx<br>$$<br>$$<br>z=\phi(\beta)=(y-X\beta)’(y-X\beta)<br>$$<br>设$e=y-X\beta$，则<br>$$<br>dz=de’e=2e’de=2e’d(y-X\beta)\\<br>=-2e’Xd\beta=-2(y-X\beta)’Xd\beta<br>$$</p>
<h2 id="3-__u96C5_u53EF_u6BD4_u77E9_u9635_28Jacobian_matrix_29"><a href="#3-__u96C5_u53EF_u6BD4_u77E9_u9635_28Jacobian_matrix_29" class="headerlink" title="3. 雅可比矩阵(Jacobian matrix)"></a>3. 雅可比矩阵(Jacobian matrix)</h2><p>　　重新梳理一下雅克比矩阵的求法，基本思路如下：<br>　　给定一个矩阵函数$F(X)$<br>　　(1)计算$F(X)$ 的微分<br>　　(2)向量化得到$d\,vec\,F(X)=A(X)d\,vec\,X$<br>　　(3)得到雅克比矩阵$DF(X)=A(X)$  </p>
<p>　　我们再次明确一下自变量和函数的符号，见下表<br>$$<br>\begin{array}{cccc}<br>\hline<br>  &amp; Scalar   &amp; Vector   &amp; Matrix   \\<br>  &amp; variable &amp; variable &amp; variable \\<br>\hline<br>Scalar\,function &amp; \phi(\xi) &amp; \phi(x) &amp; \phi(X) \\<br>Vector\,function &amp; f(\xi) &amp; f(x) &amp; f(X) \\<br>Matrix\,function &amp; F(\xi) &amp; F(x) &amp; F(X) \\<br>\hline<br>\end{array}<br>$$</p>
<p>　　雅克比矩阵本质上就是要解决如何排列 $F(X)$ 的所有偏导数 $\partial f_{st}(X)/\partial x_{ij}$ 的问题。<br>　　先给出一些符号定义：<br>　　$\phi$为标量函数，$X=(x_{ij})$ 为 $n\times q$ 矩阵，$F=(f_{st})$ 为 $m\times p$ 矩阵<br>$$<br>\frac{\partial\phi(X)}{\partial X}=<br>\left(\begin{array}{ccc}<br>  \partial\phi/\partial x_{11} &amp; \cdots &amp; \partial\phi/\partial x_{1q} \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  \partial\phi/\partial x_{n1} &amp; \cdots &amp; \partial\phi/\partial x_{nq} \\<br>\end{array}\right)<br>$$</p>
<p>$$<br>\frac{\partial F(X)}{\partial X}=<br>\left(\begin{array}{ccc}<br>  \partial f_{11}/\partial X &amp; \cdots &amp; \partial f_{1p}/\partial X \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  \partial f_{m1}/\partial X &amp; \cdots &amp; \partial f_{mp}/\partial X \\<br>\end{array}\right)<br>$$</p>
<p>　　特别地，$\phi$为标量函数，$\partial\phi/\partial x$是列向量，$\partial\phi/\partial x’$ 是行向量；$m\times1$的向量函数$f(x)$，$x$为$n\times1$向量，可以有四种排列：$\partial f/\partial x’$（$m\times n$矩阵），$\partial f’/\partial x$（$n\times m$矩阵），$\partial f/\partial x$（$mn\times 1$矩阵），$\partial f’/\partial x’$（$1\times mn$矩阵）。<br>　　下面给出雅克比矩阵的符号定义：<br>$$<br>D\phi(x)=(D_1\phi(x),\dots,D_n(x))=\frac{\partial\phi(x)}{\partial x’}<br>$$<br>$$<br>Df(x)=\frac{\partial f(x)}{\partial x’}<br>$$<br>$$<br>DF(X)=\frac{\partial\,vec\,F(X)}{\partial(vec\,X)’}<br>$$<br>　　可以对比一下，$DF(X)$ 是 $mp\times nq$，而 $\partial F(X)/\partial X$ 是$mn\times pq$。<br>　　举几个例子，$F(X)=AXB$，则<br>$$dF(X)=A(dX)B$$<br>$$d\,vec\,F(X)=(B’\otimes A)d\,vec\,X$$<br>　　因此有<br>$$DF(X)=B’\otimes A$$<br>　　令 $\phi(x)=x’Ax$，则<br>$$<br>d\phi(x)=d(x’Ax)=(dx)’Ax+x’Adx\\<br>=((dx)’Ax)’+x’Ax=x’A’dx+x’Adx\\<br>=x’(A+A’)dx<br>$$<br>　　因此有<br>$$<br>D\phi(x)=x’(A+A’)<br>$$</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/02/02/matrix_differential/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright"><div class="padding">
	
	  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>
	
	  
<div class="widget recent-post">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2017/10/21/ocpc_roi/">Paper Reading: OCPC, ROI</a>
      </li>
    
      <li>
        <a href="/2017/10/08/explore_exploit/">Paper Reading: Explore and Exploit</a>
      </li>
    
      <li>
        <a href="/2017/07/16/lambdafm/">lambdaFM</a>
      </li>
    
      <li>
        <a href="/2017/06/01/mlr_plm/">MLR, PLM</a>
      </li>
    
      <li>
        <a href="/2016/11/22/ensembling_lagrange/">Ensembling, Lagrange</a>
      </li>
    
      <li>
        <a href="/2016/10/16/fm_ftrl_softmax/">FM, FTRL, Softmax</a>
      </li>
    
      <li>
        <a href="/2016/02/05/cf_als/">CF的ALS算法推导</a>
      </li>
    
      <li>
        <a href="/2016/02/02/matrix_differential/">Matrix Differential</a>
      </li>
    
  </ul>
</div>


	
	  
	
	  
	
</div></aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="padding">
	<div class="alignleft">
	  
	  &copy; 2017 CastellanZhang
	  
	  Powerd by <a href="http://hexo.io/" target="_blank">hexo</a>
	  and Theme by <a href="https://github.com/halfer53/metro-light" target="_blank">metro-light</a>
	</div>

	<div class="alignright">
		
		
		
		
		
		
		
	</div>

	<div class="clearfix"></div>
</div>

<div class="scroll-top"><i class="fa fa-arrow-circle-up"></i></div></footer>
  


<script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/3.0.4/jquery.imagesloaded.js"></script>
<script src="/js/gallery.js"></script>



<script type="text/javascript">
$(window).scroll(function() {

    if($(this).scrollTop() > 400) {
        $('.scroll-top').fadeIn(200);
    } else {
        $('.scroll-top').fadeOut(200);
    }
});

$('.scroll-top').bind('click', function(e) {
    e.preventDefault();
    $('body,html').animate({scrollTop:0},200);
});
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
